#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
smartlearn.py - OpenClaw å¯æŽ§"è‡ªæˆ‘å­¦ä¹ "æ ¸å¿ƒï¼ˆçº¯æ ‡å‡†åº“ï¼Œæ— ä¾èµ–ï¼‰

è‡ªæˆ‘å­¦ä¹ ï¼ˆå¯è§£é‡Šã€å¯å›žæ»šï¼‰ï¼š
1) æ„å›¾è¯†åˆ«è‡ªå­¦ä¹ ï¼ˆNaive Bayesï¼‰ï¼šä½ ç”¨ /f çº æ­£ï¼Œå®ƒä¼šæŠŠæ ·æœ¬å†™å…¥ training.jsonlï¼Œå®šæœŸé‡è®­
2) å·¥å…·é€‰æ‹©è‡ªå­¦ä¹ ï¼ˆepsilon-greedyï¼‰ï¼šè®°å½• asr å¼•æ“Žï¼ˆgoogle/voskï¼‰æˆåŠŸçŽ‡ï¼Œè‡ªåŠ¨æ›´åå‘ç¨³å®šçš„
3) ç®€æ˜“çŸ¥è¯†åº“ï¼ˆTF-IDF æ£€ç´¢ + ç¼“å­˜ï¼‰ï¼š/kb æŸ¥è¯¢ï¼Œ/kbadd æ·»åŠ ï¼Œkbimport æ‰¹é‡å¯¼å…¥æ–‡ä»¶å¤¹

å®‰å…¨æ€§ï¼š
- ä¸ä¼šè‡ªæ”¹ä»£ç 
- ä¸ä¼šæ‰§è¡Œå±é™©ç³»ç»Ÿå‘½ä»¤ï¼ˆsystem_ops åªç»™"è®¡åˆ’"ï¼ŒçœŸå®žæ‰§è¡Œè¯·ä½ åšç™½åå•ï¼‰
- æ—¥å¿—è‡ªåŠ¨è„±æ•ï¼ˆé¿å…æŠŠ key/token å†™è¿›ç£ç›˜ï¼‰
"""

from __future__ import annotations

import json
import math
import random
import re
import time
from collections import Counter
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

# ====== ä½ å¯ä»¥æŒ‰ OpenClaw æ”¹è¿™äº› ======
ALLOW_INTENTS = [
    "asr_transcribe",    # è¯­éŸ³/å½•éŸ³è½¬æ–‡å­—
    "tts_speak",         # æ–‡å­—è½¬è¯­éŸ³
    "note_write",        # è®°ç¬”è®°/å½’æ¡£
    "automation_task",   # è‡ªåŠ¨åŒ–/å®šæ—¶
    "system_ops",        # ç³»ç»Ÿæ“ä½œï¼ˆå»ºè®®ä½ åšç™½åå•ï¼‰
    "question_answer",   # æ™®é€šé—®ç­”
]

CLARIFY_THRESHOLD = 0.55   # ä½ŽäºŽå°±è¿½é—®æ¾„æ¸…
EPSILON = 0.08             # å·¥å…·æŽ¢ç´¢çŽ‡ï¼ˆè¶Šå¤§è¶Šçˆ±å°è¯•ï¼‰
AUTO_TRAIN_EVERY = 5       # æ”¶é›†å¤šå°‘æ¡çº æ­£è‡ªåŠ¨é‡è®­ä¸€æ¬¡

# ====== æ•°æ®ç›®å½• ======
BASE_DIR = Path(__file__).resolve().parent / "smartlearn_data"
BASE_DIR.mkdir(parents=True, exist_ok=True)

TRAIN_FILE   = BASE_DIR / "training.jsonl"
MODEL_FILE   = BASE_DIR / "intent_model.json"
EVENTS_FILE  = BASE_DIR / "events.jsonl"
SESSION_FILE = BASE_DIR / "session.json"
PROFILE_FILE = BASE_DIR / "profile.json"
KB_FILE      = BASE_DIR / "kb_docs.jsonl"
TOOL_STATS   = BASE_DIR / "tool_stats.json"

# ====== è„±æ•ï¼ˆé¿å… key/token å†™è¿›æ—¥å¿—ï¼‰======
SENSITIVE_PATTERNS = [
    re.compile(r"\bAIza[0-9A-Za-z\-_]{25,}\b"),
    re.compile(r"\bsk-[0-9A-Za-z]{28,}\b"),
    re.compile(r"(?i)\b(token|secret|apikey|api_key)\b\s*[:=]\s*\S+"),
]


def redact(text: str) -> str:
    if not isinstance(text, str):
        return ""
    out = text
    for p in SENSITIVE_PATTERNS:
        out = p.sub("[REDACTED]", out)
    if len(out) > 4000:
        out = out[:4000] + "...[TRUNCATED]"
    return out


def tokenize(text: str) -> List[str]:
    """ä¸­æ–‡ï¼šå­— + åŒå­—ï¼›è‹±æ–‡ï¼šæŒ‰è¯ï¼›å†åŠ ç‚¹æ ‡ç‚¹ä½œä¸ºå¼±ç‰¹å¾ã€‚"""
    text = redact(text).strip()
    if not text:
        return []
    words = re.findall(r"[A-Za-z0-9_]+", text.lower())
    hans = re.findall(r"[\u4e00-\u9fff]+", text)
    zh: List[str] = []
    for seg in hans:
        zh.extend(list(seg))
        zh.extend([seg[i:i+2] for i in range(len(seg) - 1)])
    punct = re.findall(r"[ï¼Ÿ?ï¼!ã€‚.,ï¼Œ;ï¼›:ï¼š/\\\-_]", text)
    return words + zh + punct


# ====== æœ´ç´ è´å¶æ–¯æ„å›¾åˆ†ç±»ï¼ˆè‡ªå­¦ä¹ ï¼‰======
class NaiveBayesIntent:
    def __init__(self, labels: List[str]) -> None:
        self.labels = labels
        self.label_counts: Counter = Counter()
        self.token_counts: Dict[str, Counter] = {lb: Counter() for lb in labels}
        self.total_tokens: Dict[str, int] = {lb: 0 for lb in labels}
        self.vocab: set = set()
        self.trained: bool = False

    def fit(self, samples: List[Tuple[str, str]]) -> None:
        self.label_counts = Counter()
        self.token_counts = {lb: Counter() for lb in self.labels}
        self.total_tokens = {lb: 0 for lb in self.labels}
        self.vocab = set()
        for text, label in samples:
            if label not in self.labels:
                continue
            toks = tokenize(text)
            if not toks:
                continue
            self.label_counts[label] += 1
            c = Counter(toks)
            self.token_counts[label].update(c)
            self.total_tokens[label] += sum(c.values())
            self.vocab.update(c.keys())
        self.trained = sum(self.label_counts.values()) > 0

    def _log_prob(self, toks: List[str], label: str) -> float:
        alpha = 1.0
        V = max(1, len(self.vocab))
        total_docs = max(1, sum(self.label_counts.values()))
        prior = (self.label_counts[label] + alpha) / (total_docs + alpha * len(self.labels))
        logp = math.log(prior)
        denom = self.total_tokens[label] + alpha * V
        tc = self.token_counts[label]
        for t in toks:
            logp += math.log((tc.get(t, 0) + alpha) / denom)
        return logp

    def predict_proba(self, text: str) -> Dict[str, float]:
        toks = tokenize(text)
        if not toks or not self.trained:
            u = 1.0 / max(1, len(self.labels))
            return {lb: u for lb in self.labels}
        scores = {lb: self._log_prob(toks, lb) for lb in self.labels}
        m = max(scores.values())
        exps = {lb: math.exp(v - m) for lb, v in scores.items()}
        s = sum(exps.values()) or 1.0
        return {lb: exps[lb] / s for lb in self.labels}

    def predict(self, text: str) -> Tuple[str, float, List[Tuple[str, float]]]:
        proba = self.predict_proba(text)
        ranked = sorted(proba.items(), key=lambda x: x[1], reverse=True)
        return ranked[0][0], ranked[0][1], ranked

    def to_json(self) -> Dict[str, Any]:
        return {
            "labels": self.labels,
            "label_counts": dict(self.label_counts),
            "token_counts": {lb: dict(self.token_counts[lb]) for lb in self.labels},
            "total_tokens": dict(self.total_tokens),
            "vocab": list(self.vocab),
            "trained": self.trained,
        }

    @staticmethod
    def from_json(obj: Dict[str, Any]) -> "NaiveBayesIntent":
        nb = NaiveBayesIntent(obj.get("labels", []))
        nb.label_counts = Counter(obj.get("label_counts", {}))
        nb.token_counts = {lb: Counter(obj.get("token_counts", {}).get(lb, {})) for lb in nb.labels}
        nb.total_tokens = obj.get("total_tokens", {lb: 0 for lb in nb.labels})
        nb.vocab = set(obj.get("vocab", []))
        nb.trained = bool(obj.get("trained", False))
        return nb


# ====== å·¥å…·é€‰æ‹©è‡ªå­¦ä¹ ï¼ˆepsilon-greedyï¼‰======
class EpsilonGreedyChooser:
    def __init__(self, path: Path, epsilon: float = EPSILON) -> None:
        self.path = path
        self.epsilon = epsilon
        self.stats: Dict[str, Dict[str, Dict[str, int]]] = {}
        self._load()

    def _load(self) -> None:
        if self.path.exists():
            try:
                self.stats = json.loads(self.path.read_text(encoding="utf-8"))
            except Exception:
                self.stats = {}

    def _save(self) -> None:
        self.path.write_text(json.dumps(self.stats, ensure_ascii=False, indent=2), encoding="utf-8")

    def _rate(self, domain: str, option: str) -> float:
        s = self.stats.get(domain, {}).get(option, {"ok": 0, "fail": 0})
        ok, fail = s.get("ok", 0), s.get("fail", 0)
        return (ok + 1) / (ok + fail + 2)

    def choose(self, domain: str, candidates: List[str]) -> str:
        if not candidates:
            raise ValueError("no candidates")
        if random.random() < self.epsilon:
            return random.choice(candidates)
        return sorted(candidates, key=lambda c: self._rate(domain, c), reverse=True)[0]

    def update(self, domain: str, option: str, ok: bool) -> None:
        self.stats.setdefault(domain, {}).setdefault(option, {"ok": 0, "fail": 0})
        self.stats[domain][option]["ok" if ok else "fail"] += 1
        self._save()


# ====== ç®€æ˜“çŸ¥è¯†åº“ï¼ˆTF-IDF æ£€ç´¢ + ç¼“å­˜ï¼‰======
class SimpleKB:
    """
    çº¯æ ‡å‡†åº“ TF-IDFï¼š
    - kb_docs.jsonl å­˜æ–‡æ¡£
    - ç¼“å­˜ df/idf & æ¯ä¸ªæ–‡æ¡£çš„ tfï¼Œé¿å…æ¯æ¬¡æŸ¥è¯¢éƒ½é‡ç®—å…¨åº“
    """

    def __init__(self, docs_path: Path) -> None:
        self.docs_path = docs_path
        self.docs: List[Dict[str, Any]] = []
        self._doc_tfs: List[Counter] = []
        self._df: Counter = Counter()
        self._dirty: bool = True
        self._load()

    def _load(self) -> None:
        self.docs = []
        if not self.docs_path.exists():
            self._dirty = True
            return
        for line in self.docs_path.read_text(encoding="utf-8").splitlines():
            if not line.strip():
                continue
            try:
                self.docs.append(json.loads(line))
            except Exception:
                continue
        self._dirty = True

    def _rebuild_cache(self) -> None:
        self._doc_tfs = []
        self._df = Counter()
        for d in self.docs:
            toks = tokenize((d.get("title", "") or "") + "\n" + (d.get("text", "") or ""))
            tf = Counter(toks)
            self._doc_tfs.append(tf)
            for t in tf.keys():
                self._df[t] += 1
        self._dirty = False

    def add_doc(self, text: str, title: str = "", tags: Optional[List[str]] = None) -> None:
        rec = {
            "id": f"d{int(time.time() * 1000)}",
            "title": redact(title),
            "tags": tags or [],
            "text": redact(text),
            "ts": int(time.time()),
        }
        with self.docs_path.open("a", encoding="utf-8") as f:
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")
        self.docs.append(rec)
        self._dirty = True

    def import_folder(self, folder: str, exts: Tuple[str, ...] = (".md", ".txt")) -> int:
        p = Path(folder)
        if not p.exists():
            return 0
        n = 0
        for fp in p.rglob("*"):
            if fp.is_file() and fp.suffix.lower() in exts:
                try:
                    txt = fp.read_text(encoding="utf-8", errors="ignore")
                    self.add_doc(txt, title=str(fp), tags=["import"])
                    n += 1
                except Exception:
                    pass
        return n

    def _idf(self, t: str, N: int) -> float:
        return math.log((N + 1) / (self._df.get(t, 0) + 1)) + 1.0

    @staticmethod
    def _cos(a: Dict[str, float], b: Dict[str, float]) -> float:
        if not a or not b:
            return 0.0
        if len(a) > len(b):
            a, b = b, a
        dot = sum(av * b.get(t, 0.0) for t, av in a.items())
        na = math.sqrt(sum(v * v for v in a.values()))
        nb = math.sqrt(sum(v * v for v in b.values()))
        return 0.0 if na == 0 or nb == 0 else dot / (na * nb)

    def search(self, query: str, k: int = 3) -> List[Dict[str, Any]]:
        qt = tokenize(query)
        if not qt or not self.docs:
            return []
        if self._dirty:
            self._rebuild_cache()
        N = max(1, len(self.docs))
        qtf = Counter(qt)
        qvec = {t: (qtf[t] * self._idf(t, N)) for t in qtf}
        scored: List[Tuple[float, Dict[str, Any]]] = []
        for d, tf in zip(self.docs, self._doc_tfs):
            dvec = {t: (tf[t] * self._idf(t, N)) for t in tf}
            s = self._cos(qvec, dvec)
            if s > 0:
                scored.append((s, d))
        scored.sort(key=lambda x: x[0], reverse=True)
        out: List[Dict[str, Any]] = []
        for s, d in scored[:k]:
            out.append({
                "score": round(s, 4),
                "title": d.get("title", ""),
                "snippet": (d.get("text", "") or "")[:300],
                "id": d.get("id", ""),
            })
        return out


# ====== ä¸»æ ¸å¿ƒï¼šå¯åµŒå…¥ OpenClaw ======
class SelfLearningCore:
    def __init__(self) -> None:
        self.nb = NaiveBayesIntent(list(ALLOW_INTENTS))
        self.chooser = EpsilonGreedyChooser(TOOL_STATS, epsilon=EPSILON)
        self.kb = SimpleKB(KB_FILE)
        self._new_samples = 0
        self._load_model()
        self.session = self._load_json(SESSION_FILE, default={})
        self.profile = self._load_json(PROFILE_FILE, default={
            "language": "zh-CN",
            "output_style": "conclusion_steps_next",
        })

    def _load_json(self, path: Path, default: Any) -> Any:
        if path.exists():
            try:
                return json.loads(path.read_text(encoding="utf-8"))
            except Exception:
                return default
        return default

    def _save_json(self, path: Path, obj: Any) -> None:
        path.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")

    def log(self, payload: Dict[str, Any]) -> None:
        safe = {k: (redact(v) if isinstance(v, str) else v) for k, v in payload.items()}
        safe["ts"] = int(time.time())
        with EVENTS_FILE.open("a", encoding="utf-8") as f:
            f.write(json.dumps(safe, ensure_ascii=False) + "\n")

    def _load_model(self) -> None:
        if MODEL_FILE.exists():
            try:
                self.nb = NaiveBayesIntent.from_json(
                    json.loads(MODEL_FILE.read_text(encoding="utf-8"))
                )
            except Exception:
                pass

    def _save_model(self) -> None:
        MODEL_FILE.write_text(
            json.dumps(self.nb.to_json(), ensure_ascii=False, indent=2), encoding="utf-8"
        )

    def _load_training(self) -> List[Tuple[str, str]]:
        samples: List[Tuple[str, str]] = []
        if not TRAIN_FILE.exists():
            return samples
        for line in TRAIN_FILE.read_text(encoding="utf-8").splitlines():
            if not line.strip():
                continue
            try:
                obj = json.loads(line)
                t, i = obj.get("text", ""), obj.get("intent", "")
                if i in ALLOW_INTENTS and isinstance(t, str):
                    samples.append((t, i))
            except Exception:
                continue
        return samples

    def train(self) -> None:
        samples = self._load_training()
        self.nb.fit(samples)
        self._save_model()
        self.log({"type": "train", "samples": len(samples), "trained": self.nb.trained})

    def feedback(self, text: str, correct_intent: str, note: str = "") -> None:
        if correct_intent not in ALLOW_INTENTS:
            raise ValueError(f"intent not allowed: {correct_intent}")
        rec = {
            "text": redact(text),
            "intent": correct_intent,
            "note": redact(note),
            "ts": int(time.time()),
        }
        with TRAIN_FILE.open("a", encoding="utf-8") as f:
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")
        self._new_samples += 1
        self.log({"type": "feedback", "intent": correct_intent})
        if self._new_samples >= AUTO_TRAIN_EVERY:
            self.train()
            self._new_samples = 0

    def route(self, text: str) -> Dict[str, Any]:
        best, conf, ranked = self.nb.predict(text)
        top3 = ranked[:3]
        if (not self.nb.trained) or (conf < CLARIFY_THRESHOLD):
            return {"intent": "clarify", "confidence": conf, "topk": top3}
        return {"intent": best, "confidence": conf, "topk": top3}

    def clarify_msg(self, topk: List[Tuple[str, float]]) -> str:
        opts = " / ".join([f"{i+1}.{k}" for i, (k, _) in enumerate(topk)])
        return f'æˆ‘æ²¡å®Œå…¨ç¡®å®šä½ çš„æ„æ€ã€‚ä½ æ›´åƒæ˜¯è¦ï¼š{opts}ï¼Ÿå›žå¤æ•°å­— 1/2/3 æˆ–ç›´æŽ¥è¯´"æˆ‘çš„æ„æ€æ˜¯ xxx"ã€‚'

    def extract_slots(self, text: str, intent: str) -> Dict[str, Any]:
        slots: Dict[str, Any] = {}
        urls = re.findall(r"https?://\S+", text)
        if urls:
            slots["urls"] = urls
        paths = re.findall(r"[A-Za-z]:\\[^ \n\r\t]+", text)
        if paths:
            slots["paths"] = paths
        time_words = re.findall(r"(ä»Šå¤©|æ˜Žå¤©|åŽå¤©|ä»Šæ™š|æ—©ä¸Š|ä¸Šåˆ|ä¸­åˆ|ä¸‹åˆ|å‚æ™š|æ™šä¸Š)", text)
        if time_words:
            slots["time_hints"] = time_words
        if re.search(r"(è‹±æ–‡|è‹±è¯­|english)", text, re.I):
            slots["language"] = "en-US"
        else:
            slots["language"] = self.profile.get("language", "zh-CN")
        if intent == "asr_transcribe":
            if re.search(r"(ç¦»çº¿|vosk)", text, re.I):
                slots["engine"] = "vosk"
            elif re.search(r"(google|è°·æ­Œ)", text, re.I):
                slots["engine"] = "google"
            else:
                slots["engine"] = "auto"
        return slots

    def choose_asr_engine(self, prefer: str = "auto") -> str:
        if prefer in ("google", "vosk"):
            return prefer
        return self.chooser.choose("asr", ["google", "vosk"])

    def update_tool_result(self, domain: str, option: str, ok: bool) -> None:
        self.chooser.update(domain, option, ok)

    def kb_add(self, text: str, title: str = "", tags: Optional[List[str]] = None) -> None:
        self.kb.add_doc(text, title=title, tags=tags)

    def kb_search(self, query: str, k: int = 3) -> List[Dict[str, Any]]:
        return self.kb.search(query, k=k)

    def handle(self, text: str) -> Dict[str, Any]:
        text = text.strip()
        if not text:
            return {"reply": "è¯´ç‚¹ä»€ä¹ˆå§ï½ž"}

        # /f æ„å›¾ [åŽŸè¯]ï¼šçº æ­£å­¦ä¹ 
        if text.startswith("/f "):
            parts = text.split(" ", 2)
            intent = parts[1].strip() if len(parts) >= 2 else ""
            origin = parts[2].strip() if len(parts) == 3 else self.session.get("last_text", "")
            if not origin:
                return {"reply": "æ²¡æœ‰å¯çº æ­£çš„ä¸Šä¸€å¥ã€‚ç”¨ï¼š/f æ„å›¾ ä½ çš„åŽŸè¯"}
            self.feedback(origin, intent)
            return {"reply": f"å·²å­¦ä¹ ï¼š{intent}ï¼ˆä»¥åŽæ›´å‡†ï¼‰"}

        # /kbadd å†…å®¹ï¼šå†™å…¥çŸ¥è¯†åº“
        if text.startswith("/kbadd "):
            payload = text[len("/kbadd "):].strip()
            if payload:
                self.kb_add(payload, title="manual")
                return {"reply": "å·²åŠ å…¥çŸ¥è¯†åº“ã€‚"}
            return {"reply": "ç”¨æ³•ï¼š/kbadd ä½ è¦ä¿å­˜çš„å†…å®¹"}

        # /kb æŸ¥è¯¢ï¼šçŸ¥è¯†åº“æ£€ç´¢
        if text.startswith("/kb "):
            q = text[len("/kb "):].strip()
            hits = self.kb_search(q, k=3)
            if not hits:
                return {"reply": "çŸ¥è¯†åº“é‡Œæš‚æ—¶æ²¡æœåˆ°ã€‚"}
            lines = [f"- {h['title']} (score={h['score']}): {h['snippet']}" for h in hits]
            return {"reply": "æˆ‘åœ¨çŸ¥è¯†åº“é‡Œæ‰¾åˆ°ï¼š\n" + "\n".join(lines), "hits": hits}

        r = self.route(text)
        self.session["last_text"] = text
        self._save_json(SESSION_FILE, self.session)

        if r["intent"] == "clarify":
            return {"reply": self.clarify_msg(r["topk"]), "route": r}

        intent = r["intent"]
        slots = self.extract_slots(text, intent)
        plan: List[str] = []
        tool: str = ""

        if intent == "asr_transcribe":
            eng = self.choose_asr_engine(slots.get("engine", "auto"))
            tool = f"asr:{eng}"
            plan = [f"é€‰æ‹© ASR å¼•æ“Žï¼š{eng}", "èŽ·å–éŸ³é¢‘", "è½¬å†™", "è¾“å‡º/å‘é€ç»“æžœ"]
        elif intent == "note_write":
            tool = "note"
            plan = ["æ•´ç†å†…å®¹", "å†™å…¥ notes/inbox.md", "è¿”å›žä¿å­˜ä½ç½®"]
        elif intent == "tts_speak":
            tool = "tts"
            plan = ["ç”Ÿæˆè¯­éŸ³", "æ’­æ”¾/å‘é€éŸ³é¢‘"]
        elif intent == "automation_task":
            tool = "task"
            plan = ["æå–æ—¶é—´ä¸ŽåŠ¨ä½œ", "åˆ›å»º/æ›´æ–°ä»»åŠ¡", "è¿”å›žç¡®è®¤ä¿¡æ¯"]
        elif intent == "system_ops":
            tool = "system_ops_safe"
            plan = ["è¯†åˆ«ç³»ç»Ÿæ“ä½œ", "æ£€æŸ¥æ˜¯å¦åœ¨ç™½åå•", "æ‰§è¡Œ/æˆ–è¦æ±‚ç¡®è®¤", "è¿”å›žæ—¥å¿—"]
        else:
            tool = "qa"
            plan = ["å¿…è¦æ—¶æ£€ç´¢çŸ¥è¯†åº“", "ç»„ç»‡ç­”æ¡ˆ", "ç»™å‡ºä¸‹ä¸€æ­¥å»ºè®®"]

        reply = (
            f"âœ…ç»“è®ºï¼šæˆ‘ç†è§£ä½ è¦åšï¼ˆç½®ä¿¡åº¦ {r['confidence']:.2f}ï¼‰\n"
            f"ðŸ”§è®¡åˆ’ï¼š\n- " + "\n- ".join(plan) + "\n"
            f"âž¡ï¸ä¸‹ä¸€æ­¥ï¼šæŠŠæˆ‘æŽ¥åˆ°ä½ çš„å·¥å…·æ‰§è¡Œå™¨ä¸Šï¼ˆtool={tool}ï¼‰ï¼Œæˆ‘å°±èƒ½çœŸæ­£åŠ¨æ‰‹ã€‚"
        )

        self.session.update({"last_intent": intent, "last_slots": slots, "last_plan": plan})
        self._save_json(SESSION_FILE, self.session)
        self.log({"type": "route", "intent": intent, "conf": r["confidence"], "tool": tool})

        return {
            "reply": reply,
            "intent": intent,
            "slots": slots,
            "tool": tool,
            "plan": plan,
            "route": r,
        }


# ====== CLIï¼šå…ˆæœ¬åœ°è·‘é€šï¼Œå†æŽ¥ OpenClaw ======
def _cli() -> None:
    import sys

    core = SelfLearningCore()
    if len(sys.argv) <= 1:
        print('ç”¨æ³•ï¼špython smartlearn.py chat | train | route "æ–‡æœ¬" | kbimport <folder>')
        return

    cmd = sys.argv[1]
    if cmd == "train":
        core.train()
        print("trained:", core.nb.trained)
    elif cmd == "route":
        text = sys.argv[2] if len(sys.argv) > 2 else ""
        print(json.dumps(core.route(text), ensure_ascii=False, indent=2))
        print(core.handle(text)["reply"])
    elif cmd == "kbimport":
        folder = sys.argv[2] if len(sys.argv) > 2 else ""
        n = core.kb.import_folder(folder)
        print("imported:", n)
    elif cmd == "chat":
        print("è¿›å…¥äº¤äº’ï¼šè¾“å…¥æ–‡æœ¬è·¯ç”±ï¼›/f æ„å›¾ çº æ­£ï¼›/kb æŸ¥è¯¢ï¼›/kbadd åŠ çŸ¥è¯†ã€‚/q é€€å‡º")
        while True:
            text = input("> ").strip()
            if text == "/q":
                break
            out = core.handle(text)
            print(out["reply"])
    else:
        print("æœªçŸ¥å‘½ä»¤ï¼š", cmd)


if __name__ == "__main__":
    _cli()
