# scripts/voice_ptt.py - Push-to-Talk è¯­éŸ³è¾“å…¥
"""
æŒ‰ä½ Ctrl+Shift+F1 å¼€å§‹å½•éŸ³ï¼Œæ¾å¼€ç»“æŸã€‚
å½•éŸ³ â†’ faster-whisper è½¬æ–‡å­— â†’ é€šè¿‡ OpenClaw cron wake å‘é€ç»™å°ä¹ã€‚

ç”¨æ³•ï¼š
  python voice_ptt.py          # é»˜è®¤å¿«æ·é”® ctrl+shift+f1
  python voice_ptt.py --key f2 # è‡ªå®šä¹‰å¿«æ·é”®
"""
import sys, io, os, time, json, tempfile, threading, argparse
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

import sounddevice as sd
import soundfile as sf
import keyboard
import subprocess

# ---- Config ----
SAMPLE_RATE = 16000
CHANNELS = 1
WHISPER_MODEL = "large-v3"
WHISPER_DEVICE = "cuda"
WHISPER_COMPUTE = "float16"

# ---- Global state ----
recording = False
audio_frames = []
model = None  # lazy load

def load_model():
    global model
    if model is None:
        print("â³ åŠ è½½ Whisper æ¨¡å‹...")
        from faster_whisper import WhisperModel
        model = WhisperModel(WHISPER_MODEL, device=WHISPER_DEVICE, compute_type=WHISPER_COMPUTE)
        print("âœ… æ¨¡å‹å°±ç»ª")
    return model

def audio_callback(indata, frames, time_info, status):
    if recording:
        audio_frames.append(indata.copy())

def start_recording():
    global recording, audio_frames
    if recording:
        return
    audio_frames = []
    recording = True
    print("ğŸ™ï¸ å½•éŸ³ä¸­... (æ¾å¼€åœæ­¢)")

def stop_recording():
    global recording
    if not recording:
        return
    recording = False
    print("â¹ï¸ å½•éŸ³ç»“æŸï¼Œè½¬å†™ä¸­...")
    
    if not audio_frames:
        print("âš ï¸ æ²¡æœ‰å½•åˆ°éŸ³é¢‘")
        return
    
    # æ‹¼æ¥éŸ³é¢‘å¹¶ä¿å­˜ä¸´æ—¶æ–‡ä»¶
    import numpy as np
    audio_data = np.concatenate(audio_frames, axis=0)
    
    # æ£€æŸ¥éŸ³é‡ï¼ˆé™éŸ³æ£€æµ‹ï¼‰
    rms = np.sqrt(np.mean(audio_data ** 2))
    if rms < 0.005:
        print("âš ï¸ éŸ³é‡å¤ªä½ï¼Œè·³è¿‡")
        return
    
    tmp_path = os.path.join(tempfile.gettempdir(), "ptt_recording.wav")
    sf.write(tmp_path, audio_data, SAMPLE_RATE)
    
    duration = len(audio_data) / SAMPLE_RATE
    print(f"ğŸ“ éŸ³é¢‘ {duration:.1f}s, RMS={rms:.4f}")
    
    # è½¬å†™
    t0 = time.time()
    m = load_model()
    segments, info = m.transcribe(
        tmp_path,
        language="zh",
        beam_size=5,
        no_speech_threshold=0.5,
        vad_filter=True,
    )
    
    text = "".join(seg.text for seg in segments).strip()
    elapsed = time.time() - t0
    
    if not text:
        print("âš ï¸ æœªè¯†åˆ«åˆ°è¯­éŸ³")
        return
    
    print(f"ğŸ’¬ è¯†åˆ«ç»“æœ ({elapsed:.1f}s): {text}")
    
    # å‘é€ç»™ OpenClaw
    send_to_openclaw(text)
    
    # æ¸…ç†
    try:
        os.remove(tmp_path)
    except:
        pass

def send_to_openclaw(text):
    """é€šè¿‡ openclaw CLI å‘é€ wake äº‹ä»¶"""
    try:
        result = subprocess.run(
            ["openclaw", "wake", text],
            capture_output=True, text=True, timeout=10,
            encoding='utf-8', errors='replace'
        )
        if result.returncode == 0:
            print(f"âœ… å·²å‘é€ç»™å°ä¹: {text}")
        else:
            # å¤‡é€‰ï¼šå†™åˆ°æ–‡ä»¶è®©å¿ƒè·³æ¡èµ·æ¥
            fallback_path = os.path.join(
                os.path.dirname(os.path.dirname(__file__)),
                "memory", "voice_inbox.jsonl"
            )
            with open(fallback_path, "a", encoding="utf-8") as f:
                f.write(json.dumps({
                    "ts": int(time.time()),
                    "text": text,
                    "source": "ptt",
                    "delivered": False,
                }, ensure_ascii=False) + "\n")
            print(f"ğŸ“¥ å·²å­˜å…¥ voice_inbox.jsonl (openclaw wake å¤±è´¥)")
    except Exception as e:
        print(f"âš ï¸ å‘é€å¤±è´¥: {e}")

def main():
    parser = argparse.ArgumentParser(description="Push-to-Talk è¯­éŸ³è¾“å…¥")
    parser.add_argument("--key", default="ctrl+shift+f1", help="å¿«æ·é”® (é»˜è®¤ ctrl+shift+f1)")
    args = parser.parse_args()
    
    hotkey = args.key
    
    print(f"ğŸ¾ å°ä¹è¯­éŸ³è¾“å…¥ (Push-to-Talk)")
    print(f"   å¿«æ·é”®: {hotkey}")
    print(f"   æ¨¡å‹: {WHISPER_MODEL} ({WHISPER_DEVICE})")
    print(f"   æŒ‰ä½è¯´è¯ï¼Œæ¾å¼€å‘é€")
    print(f"   Ctrl+C é€€å‡º")
    print()
    
    # é¢„åŠ è½½æ¨¡å‹
    load_model()
    
    # å¼€å¯éŸ³é¢‘æµ
    stream = sd.InputStream(
        samplerate=SAMPLE_RATE,
        channels=CHANNELS,
        callback=audio_callback,
        dtype='float32',
    )
    stream.start()
    
    # æ³¨å†Œå¿«æ·é”®
    keyboard.on_press_key(hotkey.split('+')[-1], lambda e: start_recording() if keyboard.is_pressed(hotkey.rsplit('+', 1)[0]) else None)
    
    # æ›´ç®€å•çš„æ–¹å¼ï¼šç”¨ hotkey
    keyboard.add_hotkey(hotkey, start_recording, trigger_on_release=False)
    
    # æ¾å¼€æ£€æµ‹
    release_key = hotkey.split('+')[-1]
    keyboard.on_release_key(release_key, lambda e: stop_recording())
    
    print("ğŸŸ¢ å°±ç»ªï¼Œç­‰å¾…è¯­éŸ³è¾“å…¥...\n")
    
    try:
        keyboard.wait('ctrl+c')
    except KeyboardInterrupt:
        pass
    finally:
        stream.stop()
        stream.close()
        print("\nğŸ‘‹ å·²é€€å‡º")

if __name__ == "__main__":
    main()
