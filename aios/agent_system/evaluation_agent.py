#!/usr/bin/env python3
"""
AIOS Evaluation Agent - éªŒè¯å®ˆé—¨å‘˜

èŒè´£ï¼š
1. éªŒè¯ä»»ä½•æ”¹åŠ¨/ç­–ç•¥åº”ç”¨åçš„æ•ˆæœ
2. å¯¹æ¯” before/after æŒ‡æ ‡
3. ä¸è¾¾æ ‡è§¦å‘å›æ»š
4. ç”ŸæˆéªŒè¯æŠ¥å‘Š

è§¦å‘æ¡ä»¶ï¼š
- ç­–ç•¥åº”ç”¨å
- é…ç½®å˜æ›´å
- æ’ä»¶å‡çº§å
- ä¼˜åŒ–æ‰§è¡Œå

éªŒè¯é¡¹ï¼š
- æˆåŠŸç‡ï¼ˆä¸èƒ½ä¸‹é™ >10%ï¼‰
- P95 è€—æ—¶ï¼ˆä¸èƒ½å¢åŠ  >20%ï¼‰
- é”™è¯¯ç‡ï¼ˆä¸èƒ½ä¸Šå‡ >10%ï¼‰
- èµ„æºä½¿ç”¨ï¼ˆä¸èƒ½å¢åŠ  >30%ï¼‰

å·¥ä½œæµç¨‹ï¼š
1. è®°å½• baselineï¼ˆå˜æ›´å‰ï¼‰
2. ç­‰å¾…è§‚å¯ŸæœŸï¼ˆé»˜è®¤ 1 å°æ—¶ï¼‰
3. æ”¶é›† after æŒ‡æ ‡
4. å¯¹æ¯”åˆ†æ
5. åˆ¤æ–­æ˜¯å¦å›æ»š
6. ç”ŸæˆæŠ¥å‘Š

é›†æˆç‚¹ï¼š
- è¾“å…¥ï¼šchange_events.jsonlï¼ˆå˜æ›´äº‹ä»¶ï¼‰
- è¾“å‡ºï¼ševaluation_reports.jsonlï¼ˆéªŒè¯æŠ¥å‘Šï¼‰
- è§¦å‘ï¼šSchedulerï¼ˆå˜æ›´åè‡ªåŠ¨è§¦å‘ï¼‰
- å›æ»šï¼šReactorï¼ˆä¸è¾¾æ ‡æ—¶ï¼‰
"""

import json
import sys
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional

# æ·»åŠ  AIOS è·¯å¾„
AIOS_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(AIOS_ROOT))
sys.path.insert(0, str(AIOS_ROOT / "agent_system"))

from agent_tracer import TraceAnalyzer


class AIOSEvaluationAgent:
    """AIOS éªŒè¯å®ˆé—¨å‘˜ Agent"""

    # éªŒè¯é˜ˆå€¼
    SUCCESS_RATE_DROP_THRESHOLD = 0.10      # æˆåŠŸç‡ä¸‹é™ >10%
    LATENCY_INCREASE_THRESHOLD = 0.20       # è€—æ—¶å¢åŠ  >20%
    ERROR_RATE_INCREASE_THRESHOLD = 0.10    # é”™è¯¯ç‡ä¸Šå‡ >10%
    RESOURCE_INCREASE_THRESHOLD = 0.30      # èµ„æºä½¿ç”¨å¢åŠ  >30%

    # è§‚å¯ŸæœŸ
    OBSERVATION_WINDOW_HOURS = 1            # é»˜è®¤è§‚å¯Ÿ 1 å°æ—¶

    def __init__(self):
        self.data_dir = AIOS_ROOT / "agent_system" / "data"
        self.evaluation_dir = self.data_dir / "evaluations"
        self.evaluation_dir.mkdir(parents=True, exist_ok=True)
        
        self.analyzer = TraceAnalyzer()
        self.changes_file = self.data_dir / "changes.jsonl"

    def run(self, change_id: Optional[str] = None) -> Dict:
        """è¿è¡Œå®Œæ•´éªŒè¯æµç¨‹"""
        print("=" * 60)
        print("  AIOS Evaluation Agent")
        print(f"  Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)
        print()

        # å¦‚æœæ²¡æœ‰æŒ‡å®š change_idï¼ŒæŸ¥æ‰¾æœ€è¿‘çš„å˜æ›´
        if not change_id:
            change = self._find_recent_change()
            if not change:
                print("âœ… æ— å¾…éªŒè¯çš„å˜æ›´")
                return {"status": "no_changes"}
            change_id = change["id"]
        else:
            change = self._load_change(change_id)

        print(f"éªŒè¯å˜æ›´: {change_id}")
        print(f"ç±»å‹: {change['type']}")
        print(f"æè¿°: {change['description']}")
        print()

        report = {
            "timestamp": datetime.now().isoformat(),
            "change_id": change_id,
            "change": change,
            "baseline": {},
            "after": {},
            "comparison": {},
            "verdict": "unknown"
        }

        # Phase 1: åŠ è½½ baseline
        print("[Phase 1] åŠ è½½ baseline...")
        baseline = self._load_baseline(change)
        if not baseline:
            print("  âš ï¸  æ—  baselineï¼Œæ‰§è¡Œ smoke test")
            smoke_result = self._smoke_test(change)
            report["smoke_test"] = smoke_result
            report["verdict"] = "pass" if smoke_result["passed"] else "fail"
            self._save_report(report)
            return report

        report["baseline"] = baseline
        print(f"  âœ… Baseline: æˆåŠŸç‡ {baseline['success_rate']:.1%}, P95 {baseline['p95_duration']:.1f}s")

        # Phase 2: æ”¶é›† after æŒ‡æ ‡
        print("[Phase 2] æ”¶é›† after æŒ‡æ ‡...")
        after = self._collect_after_metrics(change)
        report["after"] = after
        print(f"  âœ… After: æˆåŠŸç‡ {after['success_rate']:.1%}, P95 {after['p95_duration']:.1f}s")

        # Phase 3: å¯¹æ¯”åˆ†æ
        print("[Phase 3] å¯¹æ¯”åˆ†æ...")
        comparison = self._compare_metrics(baseline, after)
        report["comparison"] = comparison
        
        # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
        for metric, result in comparison.items():
            status = "âœ…" if result["passed"] else "âŒ"
            print(f"  {status} {metric}: {result['change']:.1%}")

        # Phase 4: åˆ¤æ–­æ˜¯å¦å›æ»š
        print("[Phase 4] åˆ¤æ–­æ˜¯å¦å›æ»š...")
        verdict = self._make_verdict(comparison)
        report["verdict"] = verdict
        
        if verdict == "pass":
            print("  âœ… éªŒè¯é€šè¿‡")
        elif verdict == "fail":
            print("  âŒ éªŒè¯å¤±è´¥ï¼Œå»ºè®®å›æ»š")
            self._trigger_rollback(change, report)
        else:
            print("  âš ï¸  éœ€è¦äººå·¥åˆ¤æ–­")

        # ä¿å­˜æŠ¥å‘Š
        self._save_report(report)

        print()
        print("=" * 60)
        print(f"  éªŒè¯ç»“æœ: {verdict.upper()}")
        print("=" * 60)

        return report

    def _find_recent_change(self) -> Optional[Dict]:
        """æŸ¥æ‰¾æœ€è¿‘çš„å¾…éªŒè¯å˜æ›´"""
        if not self.changes_file.exists():
            return None

        # è¯»å–æœ€è¿‘çš„å˜æ›´
        changes = []
        with open(self.changes_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    change = json.loads(line.strip())
                    # åªçœ‹æœ€è¿‘ 2 å°æ—¶çš„å˜æ›´
                    change_time = datetime.fromisoformat(change["timestamp"])
                    if datetime.now() - change_time < timedelta(hours=2):
                        # æ£€æŸ¥æ˜¯å¦å·²éªŒè¯
                        if not change.get("evaluated", False):
                            changes.append(change)
                except:
                    continue

        return changes[-1] if changes else None

    def _load_change(self, change_id: str) -> Dict:
        """åŠ è½½å˜æ›´è®°å½•"""
        if not self.changes_file.exists():
            return None

        with open(self.changes_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    change = json.loads(line.strip())
                    if change["id"] == change_id:
                        return change
                except:
                    continue

        return None

    def _load_baseline(self, change: Dict) -> Optional[Dict]:
        """åŠ è½½ baseline æŒ‡æ ‡"""
        # ä»å˜æ›´è®°å½•ä¸­è¯»å– baseline
        baseline = change.get("baseline")
        if baseline:
            return baseline

        # å¦‚æœæ²¡æœ‰ baselineï¼Œä»å†å²æ•°æ®è®¡ç®—
        change_time = datetime.fromisoformat(change["timestamp"])
        cutoff_start = change_time - timedelta(hours=2)
        cutoff_end = change_time

        return self._calculate_metrics(cutoff_start, cutoff_end)

    def _collect_after_metrics(self, change: Dict) -> Dict:
        """æ”¶é›†å˜æ›´åçš„æŒ‡æ ‡"""
        change_time = datetime.fromisoformat(change["timestamp"])
        cutoff_start = change_time
        cutoff_end = datetime.now()

        return self._calculate_metrics(cutoff_start, cutoff_end)

    def _calculate_metrics(self, start_time: datetime, end_time: datetime) -> Dict:
        """è®¡ç®—æŒ‡æ ‡"""
        traces = [
            t for t in self.analyzer.traces
            if start_time <= datetime.fromisoformat(t.get("start_time", "")) < end_time
            and t.get("env", "prod") == "prod"
        ]

        if not traces:
            return {
                "total_tasks": 0,
                "success_rate": 0,
                "error_rate": 0,
                "p95_duration": 0,
                "avg_duration": 0
            }

        successes = sum(1 for t in traces if t.get("success", False))
        failures = len(traces) - successes

        durations = [t.get("duration_sec", 0) for t in traces if t.get("duration_sec", 0) > 0]
        durations.sort()

        p95_duration = durations[int(len(durations) * 0.95)] if durations else 0
        avg_duration = sum(durations) / len(durations) if durations else 0

        return {
            "total_tasks": len(traces),
            "success_rate": successes / len(traces) if traces else 0,
            "error_rate": failures / len(traces) if traces else 0,
            "p95_duration": p95_duration,
            "avg_duration": avg_duration
        }

    def _compare_metrics(self, baseline: Dict, after: Dict) -> Dict:
        """å¯¹æ¯”æŒ‡æ ‡"""
        comparison = {}

        # 1. æˆåŠŸç‡
        success_rate_change = after["success_rate"] - baseline["success_rate"]
        comparison["success_rate"] = {
            "baseline": baseline["success_rate"],
            "after": after["success_rate"],
            "change": success_rate_change,
            "passed": success_rate_change >= -self.SUCCESS_RATE_DROP_THRESHOLD
        }

        # 2. P95 è€—æ—¶
        if baseline["p95_duration"] > 0:
            latency_change = (after["p95_duration"] - baseline["p95_duration"]) / baseline["p95_duration"]
        else:
            latency_change = 0

        comparison["p95_duration"] = {
            "baseline": baseline["p95_duration"],
            "after": after["p95_duration"],
            "change": latency_change,
            "passed": latency_change <= self.LATENCY_INCREASE_THRESHOLD
        }

        # 3. é”™è¯¯ç‡
        error_rate_change = after["error_rate"] - baseline["error_rate"]
        comparison["error_rate"] = {
            "baseline": baseline["error_rate"],
            "after": after["error_rate"],
            "change": error_rate_change,
            "passed": error_rate_change <= self.ERROR_RATE_INCREASE_THRESHOLD
        }

        return comparison

    def _make_verdict(self, comparison: Dict) -> str:
        """åˆ¤æ–­éªŒè¯ç»“æœ"""
        # æ‰€æœ‰æŒ‡æ ‡éƒ½é€šè¿‡ â†’ pass
        if all(result["passed"] for result in comparison.values()):
            return "pass"

        # ä»»ä½•å…³é”®æŒ‡æ ‡å¤±è´¥ â†’ fail
        critical_metrics = ["success_rate", "error_rate"]
        if any(not comparison[m]["passed"] for m in critical_metrics if m in comparison):
            return "fail"

        # å…¶ä»–æƒ…å†µ â†’ needs_review
        return "needs_review"

    def _smoke_test(self, change: Dict) -> Dict:
        """Smoke testï¼ˆæ—  baseline æ—¶ï¼‰"""
        print("  æ‰§è¡Œ smoke test...")
        
        # ç®€å•æ£€æŸ¥ï¼šæœ€è¿‘ 10 åˆ†é’Ÿæœ‰æ²¡æœ‰ä¸¥é‡é”™è¯¯
        cutoff = datetime.now() - timedelta(minutes=10)
        recent_traces = [
            t for t in self.analyzer.traces
            if datetime.fromisoformat(t.get("start_time", "")) >= cutoff
            and t.get("env", "prod") == "prod"
        ]

        if not recent_traces:
            return {"passed": True, "reason": "æ— æ•°æ®ï¼Œé»˜è®¤é€šè¿‡"}

        failures = sum(1 for t in recent_traces if not t.get("success", False))
        failure_rate = failures / len(recent_traces)

        if failure_rate > 0.5:
            return {"passed": False, "reason": f"å¤±è´¥ç‡è¿‡é«˜: {failure_rate:.1%}"}

        return {"passed": True, "reason": "smoke test é€šè¿‡"}

    def _trigger_rollback(self, change: Dict, report: Dict):
        """è§¦å‘å›æ»š"""
        print("  ğŸ”„ è§¦å‘å›æ»š...")
        
        # ç”Ÿæˆå›æ»šäº‹ä»¶
        rollback_event = {
            "type": "rollback_requested",
            "timestamp": datetime.now().isoformat(),
            "change_id": change["id"],
            "reason": "éªŒè¯å¤±è´¥",
            "report": report
        }

        # å†™å…¥äº‹ä»¶æµï¼ˆReactor ä¼šå¤„ç†ï¼‰
        events_file = AIOS_ROOT / "data" / "events.jsonl"
        with open(events_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(rollback_event, ensure_ascii=False) + '\n')

        print("  âœ… å›æ»šè¯·æ±‚å·²å‘é€")

    def _save_report(self, report: Dict):
        """ä¿å­˜éªŒè¯æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.evaluation_dir / f"eval_{timestamp}.json"
        
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

        # æ ‡è®°å˜æ›´å·²éªŒè¯
        if self.changes_file.exists():
            self._mark_change_evaluated(report["change_id"])

    def _mark_change_evaluated(self, change_id: str):
        """æ ‡è®°å˜æ›´å·²éªŒè¯"""
        if not self.changes_file.exists():
            return

        lines = []
        with open(self.changes_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    change = json.loads(line.strip())
                    if change["id"] == change_id:
                        change["evaluated"] = True
                        change["evaluated_at"] = datetime.now().isoformat()
                    lines.append(json.dumps(change, ensure_ascii=False))
                except:
                    lines.append(line.strip())

        with open(self.changes_file, 'w', encoding='utf-8') as f:
            for line in lines:
                f.write(line + '\n')


def main():
    """ä¸»å‡½æ•°"""
    agent = AIOSEvaluationAgent()
    report = agent.run()
    
    # è¾“å‡ºæ‘˜è¦
    status = report.get("status")
    if status == "no_changes":
        print("\nEVALUATION_OK")
    else:
        verdict = report.get("verdict", "unknown")
        if verdict == "pass":
            print("\nEVALUATION_PASS")
        elif verdict == "fail":
            print("\nEVALUATION_FAIL")
        else:
            print("\nEVALUATION_REVIEW")


if __name__ == "__main__":
    main()
