#!/usr/bin/env python3
"""
Evaluator Agent - è¯„æµ‹ä¸å›å½’æµ‹è¯•
å›ºå®šæµ‹è¯•é›† + æ€§èƒ½å¯¹æ¯” + é˜²æ­¢é€€åŒ–
"""
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List
from data_collector import DataCollector

# æµ‹è¯•é›†ç›®å½•
TEST_SUITE_DIR = Path(__file__).parent / "test_suite"
TEST_SUITE_DIR.mkdir(exist_ok=True)

# æµ‹è¯•ç»“æœç›®å½•
RESULTS_DIR = Path(__file__).parent / "data" / "evaluation"
RESULTS_DIR.mkdir(parents=True, exist_ok=True)

# å›ºå®šæµ‹è¯•é›†
DEFAULT_TEST_SUITE = [
    {
        "id": "test_001",
        "name": "ç®€å•å‡½æ•°",
        "description": "å†™ä¸€ä¸ªå‡½æ•°è®¡ç®—1åˆ°nçš„å’Œ",
        "type": "code",
        "priority": "normal",
        "expected": {
            "success": True,
            "has_function": True,
            "has_error_handling": True,
            "max_duration_ms": 10000
        }
    },
    {
        "id": "test_002",
        "name": "æ–æ³¢é‚£å¥‘æ•°åˆ—",
        "description": "å†™ä¸€ä¸ªå‡½æ•°ï¼Œè¾“å…¥nï¼Œè¿”å›æ–æ³¢é‚£å¥‘æ•°åˆ—å‰né¡¹",
        "type": "code",
        "priority": "normal",
        "expected": {
            "success": True,
            "has_function": True,
            "max_duration_ms": 10000
        }
    },
    {
        "id": "test_003",
        "name": "è´¨æ•°åˆ¤æ–­",
        "description": "å†™ä¸€ä¸ªå‡½æ•°åˆ¤æ–­ä¸€ä¸ªæ•°æ˜¯å¦ä¸ºè´¨æ•°",
        "type": "code",
        "priority": "normal",
        "expected": {
            "success": True,
            "has_function": True,
            "has_optimization": False,  # ä¸å¼ºåˆ¶è¦æ±‚ä¼˜åŒ–
            "max_duration_ms": 10000
        }
    },
    {
        "id": "test_004",
        "name": "ç®€å•çˆ¬è™«",
        "description": "å†™ä¸€ä¸ªçˆ¬è™«æŠ“å–ç½‘é¡µæ ‡é¢˜",
        "type": "code",
        "priority": "high",
        "expected": {
            "success": True,
            "has_error_handling": True,
            "max_duration_ms": 15000
        }
    },
    {
        "id": "test_005",
        "name": "Flask API",
        "description": "å†™ä¸€ä¸ªç®€å•çš„ Flask APIï¼Œè¿”å›å½“å‰æ—¶é—´",
        "type": "code",
        "priority": "high",
        "expected": {
            "success": True,
            "has_error_handling": True,
            "max_duration_ms": 15000
        }
    }
]

class Evaluator:
    """è¯„æµ‹å™¨"""
    
    def __init__(self):
        self.test_suite = self._load_test_suite()
    
    def _load_test_suite(self) -> List[Dict]:
        """åŠ è½½æµ‹è¯•é›†"""
        test_file = TEST_SUITE_DIR / "test_suite.json"
        
        if test_file.exists():
            with open(test_file, encoding="utf-8") as f:
                return json.load(f)
        else:
            # åˆ›å»ºé»˜è®¤æµ‹è¯•é›†
            with open(test_file, "w", encoding="utf-8") as f:
                json.dump(DEFAULT_TEST_SUITE, f, indent=2, ensure_ascii=False)
            return DEFAULT_TEST_SUITE
    
    def run_test(self, test_case: Dict) -> Dict:
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        print(f"\nğŸ§ª æµ‹è¯•: {test_case['name']}")
        print(f"   æè¿°: {test_case['description']}")
        
        start_time = time.time()
        
        # è¿™é‡Œåº”è¯¥è°ƒç”¨ real_coder æ‰§è¡Œä»»åŠ¡
        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿæ‰§è¡Œ
        result = {
            "test_id": test_case["id"],
            "test_name": test_case["name"],
            "status": "success",  # æ¨¡æ‹ŸæˆåŠŸ
            "duration_ms": int((time.time() - start_time) * 1000),
            "timestamp": datetime.now().isoformat(),
            "metrics": {
                "has_function": True,
                "has_error_handling": True,
                "code_length": 150,
                "execution_success": True
            }
        }
        
        # éªŒè¯æœŸæœ›
        passed = self._validate_expectations(result, test_case["expected"])
        result["passed"] = passed
        
        if passed:
            print(f"   âœ… é€šè¿‡")
        else:
            print(f"   âŒ å¤±è´¥")
        
        return result
    
    def _validate_expectations(self, result: Dict, expected: Dict) -> bool:
        """éªŒè¯æ˜¯å¦ç¬¦åˆæœŸæœ›"""
        if not result["status"] == "success":
            return False
        
        metrics = result["metrics"]
        
        # æ£€æŸ¥å„é¡¹æœŸæœ›
        if expected.get("has_function") and not metrics.get("has_function"):
            return False
        
        if expected.get("has_error_handling") and not metrics.get("has_error_handling"):
            return False
        
        if expected.get("max_duration_ms") and result["duration_ms"] > expected["max_duration_ms"]:
            return False
        
        return True
    
    def run_suite(self) -> Dict:
        """è¿è¡Œå®Œæ•´æµ‹è¯•é›†"""
        print("=" * 80)
        print("ğŸš€ å¼€å§‹å›å½’æµ‹è¯•")
        print("=" * 80)
        
        results = []
        passed = 0
        failed = 0
        
        for test_case in self.test_suite:
            result = self.run_test(test_case)
            results.append(result)
            
            if result["passed"]:
                passed += 1
            else:
                failed += 1
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "timestamp": datetime.now().isoformat(),
            "total": len(self.test_suite),
            "passed": passed,
            "failed": failed,
            "success_rate": (passed / len(self.test_suite)) * 100,
            "results": results
        }
        
        # ä¿å­˜ç»“æœ
        result_file = RESULTS_DIR / f"result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(result_file, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print("\n" + "=" * 80)
        print("ğŸ“Š æµ‹è¯•å®Œæˆ")
        print("=" * 80)
        print(f"æ€»è®¡: {report['total']}")
        print(f"é€šè¿‡: {report['passed']}")
        print(f"å¤±è´¥: {report['failed']}")
        print(f"æˆåŠŸç‡: {report['success_rate']:.1f}%")
        print(f"\nç»“æœå·²ä¿å­˜: {result_file}")
        
        return report
    
    def compare_results(self, baseline_file: str = None) -> Dict:
        """å¯¹æ¯”ä¸¤æ¬¡æµ‹è¯•ç»“æœ"""
        # è·å–æœ€æ–°çš„ä¸¤ä¸ªç»“æœæ–‡ä»¶
        result_files = sorted(RESULTS_DIR.glob("result_*.json"), reverse=True)
        
        if len(result_files) < 2:
            print("âš ï¸ éœ€è¦è‡³å°‘2æ¬¡æµ‹è¯•ç»“æœæ‰èƒ½å¯¹æ¯”")
            return None
        
        # è¯»å–æœ€æ–°å’ŒåŸºçº¿ç»“æœ
        with open(result_files[0], encoding="utf-8") as f:
            current = json.load(f)
        
        with open(result_files[1], encoding="utf-8") as f:
            baseline = json.load(f)
        
        # å¯¹æ¯”
        comparison = {
            "current_timestamp": current["timestamp"],
            "baseline_timestamp": baseline["timestamp"],
            "success_rate_change": current["success_rate"] - baseline["success_rate"],
            "passed_change": current["passed"] - baseline["passed"],
            "failed_change": current["failed"] - baseline["failed"],
            "regression": current["success_rate"] < baseline["success_rate"]
        }
        
        print("\n" + "=" * 80)
        print("ğŸ“ˆ æ€§èƒ½å¯¹æ¯”")
        print("=" * 80)
        print(f"åŸºçº¿: {baseline['timestamp']}")
        print(f"å½“å‰: {current['timestamp']}")
        print(f"\næˆåŠŸç‡: {baseline['success_rate']:.1f}% â†’ {current['success_rate']:.1f}% ({comparison['success_rate_change']:+.1f}%)")
        print(f"é€šè¿‡: {baseline['passed']} â†’ {current['passed']} ({comparison['passed_change']:+d})")
        print(f"å¤±è´¥: {baseline['failed']} â†’ {current['failed']} ({comparison['failed_change']:+d})")
        
        if comparison["regression"]:
            print("\nâš ï¸ æ£€æµ‹åˆ°æ€§èƒ½é€€åŒ–ï¼")
        else:
            print("\nâœ… æ€§èƒ½ä¿æŒæˆ–æå‡")
        
        return comparison
    
    def generate_report(self) -> str:
        """ç”Ÿæˆè¯„æµ‹æŠ¥å‘Š"""
        # è·å–æœ€æ–°ç»“æœ
        result_files = sorted(RESULTS_DIR.glob("result_*.json"), reverse=True)
        
        if not result_files:
            return "æš‚æ— æµ‹è¯•ç»“æœ"
        
        with open(result_files[0], encoding="utf-8") as f:
            latest = json.load(f)
        
        report = f"""
ğŸ“Š è¯„æµ‹æŠ¥å‘Š - {latest['timestamp']}

âœ… é€šè¿‡: {latest['passed']}/{latest['total']} ({latest['success_rate']:.1f}%)
âŒ å¤±è´¥: {latest['failed']}/{latest['total']}

è¯¦ç»†ç»“æœ:
"""
        for result in latest['results']:
            status = "âœ…" if result['passed'] else "âŒ"
            report += f"{status} {result['test_name']}: {result['duration_ms']}ms\n"
        
        return report

# ä¾¿æ·å‡½æ•°
def run_evaluation():
    """è¿è¡Œè¯„æµ‹"""
    evaluator = Evaluator()
    return evaluator.run_suite()

def compare_performance():
    """å¯¹æ¯”æ€§èƒ½"""
    evaluator = Evaluator()
    return evaluator.compare_results()

if __name__ == "__main__":
    # æµ‹è¯•
    print("ğŸ§ª Evaluator Agent æµ‹è¯•\n")
    
    evaluator = Evaluator()
    
    # è¿è¡Œæµ‹è¯•é›†
    report = evaluator.run_suite()
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\n" + evaluator.generate_report())
