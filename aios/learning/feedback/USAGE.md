# 反馈闭环系统使用指南

## 快速开始

### 1. 给反馈的方式

**最简单：直接说**
```
你：帮我打开 QQ音乐
我：已打开 QQ音乐
你：有用
```

**关键词列表：**
- ✅ 正面：有用、好、不错、可以、行、👍、赞、很好、完美、太棒
- ❌ 负面：没用、不好、别、不要、👎、差、烦、吵、不需要

### 2. 系统会自动做什么

**记录反馈：**
- 检测到关键词后，自动关联到最近的建议/提醒
- 记录到 `feedback.jsonl`
- 更新统计数据

**学习改进：**
- 统计各类建议的接受率
- 识别你喜欢/不喜欢的建议类型
- 自动调整建议策略

### 3. 查看反馈统计

```bash
# 查看最近 7 天的反馈统计
python -m aios.learning.feedback.tracker stats

# 查看最近 30 天
python -m aios.learning.feedback.tracker stats 30
```

输出示例：
```
📊 反馈统计（最近 7 天）

总反馈数：15
接受率：73%

按类别：
  habit_suggestion: 7/10 (70%)
  health_reminder: 4/5 (80%)

按行动类型：
  suggestion: 11/15 (73%)
  reminder: 4/5 (80%)
```

## 使用场景

### 场景 1：习惯建议
```
我：通常这个时间你会听音乐，要不要打开 QQ音乐？
你：好
→ 记录为正反馈，未来会更频繁地在这个时间建议
```

### 场景 2：健康提醒
```
我：你已经连续玩了 3 小时 LOL，建议休息 10 分钟
你：别烦
→ 记录为负反馈，未来会减少这类提醒
```

### 场景 3：效率建议
```
我：检测到频繁切换应用，当前可能不是最佳工作时间
你：有道理
→ 记录为正反馈，未来会继续这类观察
```

## 系统如何学习

### 第 1 周：收集数据
- 记录所有反馈
- 不做任何调整
- 积累基础数据

### 第 2-3 周：识别模式
- 统计各类建议的接受率
- 识别时间/场景模式
- 生成初步规则

### 第 4 周+：自动优化
- 应用学到的规则
- 高接受率的建议 → 增加频率
- 低接受率的建议 → 减少频率
- 持续调整优化

## 学到的规则示例

```json
{
  "rule_id": "lr-001",
  "description": "珊瑚海不喜欢工作时间被打断玩游戏的建议",
  "condition": {
    "time_range": ["09:00", "18:00"],
    "weekday": [0, 1, 2, 3, 4],
    "action_type": "gaming_suggestion"
  },
  "action": "suppress",
  "confidence": 0.85,
  "evidence_count": 6
}
```

**效果：**
- 工作日 9:00-18:00 不再建议玩游戏
- 基于 6 次负反馈学到的规则
- 置信度 85%

## 隐私说明

- ✅ 所有数据本地存储
- ✅ 不上传任何信息
- ✅ 可随时查看/删除
- ✅ 不记录敏感内容

## 数据文件位置

```
aios/learning/feedback/data/
├── feedback.jsonl          # 反馈记录
├── feedback_stats.json     # 统计汇总
├── learned_rules.json      # 学到的规则
└── tracker_state.json      # 追踪器状态
```

## 常见问题

### Q: 我不想给反馈怎么办？
A: 没关系！系统也会从隐式信号中学习（比如你是否采纳了建议）

### Q: 我给错反馈了怎么办？
A: 没关系，单次反馈影响很小。系统需要多次证据才会形成规则。

### Q: 系统会不会学错？
A: 有可能。但规则有置信度，低置信度的规则会自动删除。你也可以手动查看和删除规则。

### Q: 多久能看到效果？
A: 通常 2-3 周后，系统会开始应用学到的规则。

## 下一步

- ✅ Phase 1 完成：基础反馈收集
- ⏳ Phase 2（1 周后）：统计分析
- ⏳ Phase 3（2 周后）：规则学习

---

**开始时间**：2026-02-23  
**当前状态**：Phase 1 运行中  
**下次检查**：2026-03-02（1 周后）
