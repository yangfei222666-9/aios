"""
AIOS Ollama é›†æˆé…ç½®

M2 MacBook ä¸Šçš„ Ollama æœåŠ¡é…ç½®
"""

import requests
import json
from typing import Dict, Any, Optional


class OllamaClient:
    """Ollama API å®¢æˆ·ç«¯"""
    
    def __init__(self, base_url: str = "http://localhost:11434"):
        """
        åˆå§‹åŒ– Ollama å®¢æˆ·ç«¯
        
        Args:
            base_url: Ollama API åœ°å€ï¼ˆM2 MacBook çš„ IPï¼‰
        """
        self.base_url = base_url
        self.api_url = f"{base_url}/api"
    
    def list_models(self) -> Dict[str, Any]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
        try:
            response = requests.get(f"{self.api_url}/tags", timeout=5)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            return {"error": str(e)}
    
    def generate(self, model: str, prompt: str, stream: bool = False) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ–‡æœ¬
        
        Args:
            model: æ¨¡å‹åç§°ï¼ˆä¾‹å¦‚ï¼šgemma3:4b, qwen2.5:7bï¼‰
            prompt: æç¤ºè¯
            stream: æ˜¯å¦æµå¼è¾“å‡º
        
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        try:
            payload = {
                "model": model,
                "prompt": prompt,
                "stream": stream
            }
            
            response = requests.post(
                f"{self.api_url}/generate",
                json=payload,
                timeout=60
            )
            response.raise_for_status()
            
            if stream:
                # æµå¼è¾“å‡º
                result = ""
                for line in response.iter_lines():
                    if line:
                        data = json.loads(line)
                        if 'response' in data:
                            result += data['response']
                return {"response": result}
            else:
                # éæµå¼è¾“å‡º
                return response.json()
        
        except Exception as e:
            return {"error": str(e)}
    
    def chat(self, model: str, messages: list, stream: bool = False) -> Dict[str, Any]:
        """
        å¯¹è¯æ¨¡å¼
        
        Args:
            model: æ¨¡å‹åç§°
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
            stream: æ˜¯å¦æµå¼è¾“å‡º
        
        Returns:
            å¯¹è¯å“åº”
        """
        try:
            payload = {
                "model": model,
                "messages": messages,
                "stream": stream
            }
            
            response = requests.post(
                f"{self.api_url}/chat",
                json=payload,
                timeout=60
            )
            response.raise_for_status()
            
            if stream:
                # æµå¼è¾“å‡º
                result = ""
                for line in response.iter_lines():
                    if line:
                        data = json.loads(line)
                        if 'message' in data and 'content' in data['message']:
                            result += data['message']['content']
                return {"message": {"content": result}}
            else:
                # éæµå¼è¾“å‡º
                return response.json()
        
        except Exception as e:
            return {"error": str(e)}


def test_ollama_connection():
    """æµ‹è¯• Ollama è¿æ¥"""
    print("\n=== æµ‹è¯• Ollama è¿æ¥ ===\n")
    
    client = OllamaClient()
    
    # 1. åˆ—å‡ºæ¨¡å‹
    print("1. åˆ—å‡ºå¯ç”¨æ¨¡å‹...")
    models = client.list_models()
    if "error" in models:
        print(f"   âŒ è¿æ¥å¤±è´¥: {models['error']}")
        print("\nè¯·ç¡®ä¿ï¼š")
        print("   1. M2 MacBook ä¸Šè¿è¡Œäº† 'ollama serve'")
        print("   2. M2 å’Œ Windows åœ¨åŒä¸€ç½‘ç»œ")
        print("   3. M2 çš„é˜²ç«å¢™å…è®¸ç«¯å£ 11434")
        return False
    
    print("   âœ… è¿æ¥æˆåŠŸï¼")
    print(f"   å¯ç”¨æ¨¡å‹: {len(models.get('models', []))} ä¸ª")
    for model in models.get('models', []):
        print(f"      - {model['name']}")
    print()
    
    # 2. æµ‹è¯•ç”Ÿæˆ
    if models.get('models'):
        model_name = models['models'][0]['name']
        print(f"2. æµ‹è¯•ç”Ÿæˆï¼ˆæ¨¡å‹: {model_name}ï¼‰...")
        result = client.generate(model_name, "Say hello in one sentence")
        
        if "error" in result:
            print(f"   âŒ ç”Ÿæˆå¤±è´¥: {result['error']}")
            return False
        
        print(f"   âœ… ç”ŸæˆæˆåŠŸï¼")
        print(f"   å“åº”: {result.get('response', '')[:100]}...")
        print()
    
    # 3. æµ‹è¯•å¯¹è¯
    if models.get('models'):
        model_name = models['models'][0]['name']
        print(f"3. æµ‹è¯•å¯¹è¯ï¼ˆæ¨¡å‹: {model_name}ï¼‰...")
        messages = [
            {"role": "user", "content": "What is 1+1?"}
        ]
        result = client.chat(model_name, messages)
        
        if "error" in result:
            print(f"   âŒ å¯¹è¯å¤±è´¥: {result['error']}")
            return False
        
        print(f"   âœ… å¯¹è¯æˆåŠŸï¼")
        print(f"   å“åº”: {result.get('message', {}).get('content', '')[:100]}...")
        print()
    
    print("=== æ‰€æœ‰æµ‹è¯•é€šè¿‡ âœ… ===\n")
    return True


if __name__ == '__main__':
    # æµ‹è¯•è¿æ¥
    success = test_ollama_connection()
    
    if success:
        print("\nğŸ‰ Ollama é›†æˆé…ç½®æˆåŠŸï¼\n")
        print("ç°åœ¨å¯ä»¥åœ¨ AIOS ä¸­ä½¿ç”¨ M2 ä¸Šçš„æ¨¡å‹äº†ï¼")
        print("\nä½¿ç”¨ç¤ºä¾‹ï¼š")
        print("```python")
        print("from ollama_client import OllamaClient")
        print("")
        print("client = OllamaClient()")
        print("result = client.generate('gemma3:4b', 'å†™ä¸€ä¸ª Python å‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—')")
        print("print(result['response'])")
        print("```")
    else:
        print("\nâŒ è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
