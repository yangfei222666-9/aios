#!/usr/bin/env python3
"""
AIOS è®°å¿†ç›²åŒºåˆ†æ v1.0
æ£€æµ‹çŸ¥è¯†ç¼ºå¤±ã€æä¾›ä¿®å¤å»ºè®®ã€è¶…é˜ˆå€¼è‡ªåŠ¨æé†’

ç”¨æ³•:
  python -m aios.scripts.memory_gaps                    # åˆ†æè¿‡å»7å¤©
  python -m aios.scripts.memory_gaps --days 3           # è‡ªå®šä¹‰å¤©æ•°
  python -m aios.scripts.memory_gaps --format telegram  # ç²¾ç®€ç‰ˆ
  python -m aios.scripts.memory_gaps --save             # ä¿å­˜æŠ¥å‘Š
"""

import json, sys, time, argparse
from collections import Counter, defaultdict
from datetime import datetime
from pathlib import Path

sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from core.engine import load_events, VALID_LAYERS
from core.config import get_path

WORKSPACE = Path.home() / ".openclaw" / "workspace"
MEMORY_DIR = WORKSPACE / "memory"
REPORTS_DIR = Path(__file__).resolve().parent.parent / "reports"
SUGGESTIONS_LOG = (
    Path(__file__).resolve().parent.parent / "events" / "gap_suggestions.jsonl"
)

# é˜ˆå€¼
ALERT_THRESHOLD = 3  # ç›²åŒºæ•° >= 3 æ—¶è§¦å‘æé†’
REPEAT_THRESHOLD = 2  # åŒä¸€çŸ¥è¯†ç‚¹ miss >= 2 æ¬¡è§†ä¸ºé«˜é¢‘ç›²åŒº


def _payload(e: dict) -> dict:
    return e.get("payload", e.get("data", {}))


def _event_name(e: dict) -> str:
    return e.get("event", _payload(e).get("_v1_type", e.get("type", "?")))


def _extract_topic(e: dict) -> str:
    """ä» MEM äº‹ä»¶ä¸­æå–çŸ¥è¯†ä¸»é¢˜"""
    p = _payload(e)
    # å°è¯•å¤šç§å­—æ®µ
    topic = (
        p.get("query")
        or p.get("topic")
        or p.get("key")
        or p.get("_v1_summary")
        or p.get("search_term")
        or ""
    )
    if not topic:
        # ä»äº‹ä»¶åæ¨æ–­
        name = _event_name(e)
        if "miss" in name:
            topic = name.replace("memory_miss_", "").replace("_miss", "")
    return str(topic).strip()[:100]


def _extract_context(e: dict) -> str:
    """æå–äº‹ä»¶ä¸Šä¸‹æ–‡ï¼ˆç”¨äºä¿®å¤å»ºè®®ï¼‰"""
    p = _payload(e)
    ctx = p.get("context") or p.get("source") or p.get("_v1_source") or ""
    return str(ctx).strip()[:200]


def _log_suggestions(suggestions: list):
    """è½ç›˜å½“å‰å»ºè®®ï¼Œä¾›åç»­å‘½ä¸­ç‡è®¡ç®—"""
    if not suggestions:
        return
    SUGGESTIONS_LOG.parent.mkdir(parents=True, exist_ok=True)
    now = time.time()
    ts = time.strftime("%Y-%m-%dT%H:%M:%S")
    with SUGGESTIONS_LOG.open("a", encoding="utf-8") as f:
        for s in suggestions:
            entry = {
                "ts": ts,
                "epoch": int(now),
                "topic": s["topic"],
                "priority": s["priority"],
                "action": s["action"],
            }
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")


def _compute_hit_rate(current_gaps: Counter) -> dict:
    """
    è®¡ç®—ä¿®å¤å‘½ä¸­ç‡ï¼šä¹‹å‰å»ºè®®è¿‡çš„ä¸»é¢˜ï¼Œå½“å‰ miss æ˜¯å¦ä¸‹é™ã€‚
    è¿”å› {total_suggested, fixed, still_open, hit_rate_pct}
    """
    if not SUGGESTIONS_LOG.exists():
        return {"total_suggested": 0, "fixed": 0, "still_open": 0, "hit_rate_pct": None}

    # åŠ è½½å†å²å»ºè®®ï¼ˆå»é‡ï¼Œåªçœ‹ 7 å¤©å‰çš„ï¼Œç»™ä¿®å¤ç•™æ—¶é—´ï¼‰
    cutoff = time.time() - 7 * 86400
    past_topics = set()
    try:
        for line in SUGGESTIONS_LOG.read_text(encoding="utf-8").splitlines():
            if not line.strip():
                continue
            entry = json.loads(line)
            if entry.get("epoch", 0) < cutoff:
                past_topics.add(entry.get("topic", ""))
    except:
        return {"total_suggested": 0, "fixed": 0, "still_open": 0, "hit_rate_pct": None}

    if not past_topics:
        return {"total_suggested": 0, "fixed": 0, "still_open": 0, "hit_rate_pct": None}

    # å¯¹æ¯”ï¼šä¹‹å‰å»ºè®®çš„ä¸»é¢˜ï¼Œç°åœ¨è¿˜åœ¨ miss å—ï¼Ÿ
    fixed = 0
    still_open = 0
    for topic in past_topics:
        if topic in current_gaps and current_gaps[topic] > 0:
            still_open += 1
        else:
            fixed += 1

    total = len(past_topics)
    rate = round(fixed / total * 100, 1) if total > 0 else 0

    return {
        "total_suggested": total,
        "fixed": fixed,
        "still_open": still_open,
        "hit_rate_pct": rate,
    }


def scan_memory_files() -> dict:
    """æ‰«æç°æœ‰è®°å¿†æ–‡ä»¶ï¼Œè¿”å›çŸ¥è¯†è¦†ç›–æ¦‚å†µ"""
    info = {
        "memory_md_exists": (WORKSPACE / "MEMORY.md").exists(),
        "memory_md_size": 0,
        "daily_files": [],
        "total_daily_size": 0,
        "lessons_exists": (MEMORY_DIR / "lessons.json").exists(),
        "corrections_exists": (MEMORY_DIR / "corrections.json").exists(),
    }

    if info["memory_md_exists"]:
        info["memory_md_size"] = (WORKSPACE / "MEMORY.md").stat().st_size

    if MEMORY_DIR.exists():
        for f in sorted(MEMORY_DIR.glob("202*.md")):
            size = f.stat().st_size
            info["daily_files"].append({"name": f.name, "size": size})
            info["total_daily_size"] += size

    return info


def analyze_gaps(days: int = 7) -> dict:
    """åˆ†æè®°å¿†ç›²åŒº"""
    events = load_events(days, layer="MEM")

    # åˆ†ç±» MEM äº‹ä»¶
    reads = []
    writes = []
    misses = []

    for e in events:
        name = _event_name(e)
        if any(k in name for k in ("miss", "not_found", "gap")):
            misses.append(e)
        elif any(
            k in name for k in ("recall", "match", "confirm", "load", "search", "read")
        ):
            reads.append(e)
        elif any(
            k in name for k in ("store", "correction", "lesson", "write", "update")
        ):
            writes.append(e)

    # æå–ç›²åŒºä¸»é¢˜
    gap_topics = Counter()
    gap_details = defaultdict(list)
    for e in misses:
        topic = _extract_topic(e)
        if topic:
            gap_topics[topic] += 1
            gap_details[topic].append(
                {
                    "ts": e.get("ts", "?"),
                    "context": _extract_context(e),
                    "event": _event_name(e),
                }
            )

    # é«˜é¢‘ç›²åŒºï¼ˆåŒä¸€ä¸»é¢˜ miss å¤šæ¬¡ï¼‰
    high_freq = {t: c for t, c in gap_topics.items() if c >= REPEAT_THRESHOLD}

    # è¯»å†™æ¯”åˆ†æ
    read_count = len(reads)
    write_count = len(writes)
    miss_count = len(misses)
    ratio = read_count / max(write_count, 1)

    # çŸ¥è¯†è¦†ç›–
    read_topics = Counter(_extract_topic(e) for e in reads if _extract_topic(e))
    write_topics = Counter(_extract_topic(e) for e in writes if _extract_topic(e))
    # è¯»äº†ä½†ä»æ²¡å†™è¿‡çš„ = æ½œåœ¨ç›²åŒº
    read_only = set(read_topics.keys()) - set(write_topics.keys())

    # ä¿®å¤å»ºè®®
    suggestions = []
    for topic, count in sorted(gap_topics.items(), key=lambda x: -x[1]):
        if not topic:
            continue
        details = gap_details[topic]
        contexts = [d["context"] for d in details if d["context"]]
        ctx_str = f"ï¼ˆæ¥æº: {contexts[0]}ï¼‰" if contexts else ""
        if count >= REPEAT_THRESHOLD:
            suggestions.append(
                {
                    "priority": "é«˜",
                    "topic": topic,
                    "reason": f"é‡å¤ miss {count} æ¬¡{ctx_str}",
                    "action": f"å°†ã€Œ{topic}ã€ç›¸å…³çŸ¥è¯†è¡¥å½•åˆ° MEMORY.md æˆ– lessons.json",
                }
            )
        else:
            suggestions.append(
                {
                    "priority": "ä¸­",
                    "topic": topic,
                    "reason": f"miss {count} æ¬¡{ctx_str}",
                    "action": f"ä¸‹æ¬¡é‡åˆ°æ—¶ä¸»åŠ¨è®°å½•ã€Œ{topic}ã€",
                }
            )

    # è¯»å¤šå†™å°‘çš„æ½œåœ¨ç›²åŒº
    for topic in sorted(read_only):
        if topic and read_topics[topic] >= 3:
            suggestions.append(
                {
                    "priority": "ä½",
                    "topic": topic,
                    "reason": f"è¯»å– {read_topics[topic]} æ¬¡ä½†ä»æœªå†™å…¥",
                    "action": f"è€ƒè™‘å°†ã€Œ{topic}ã€çš„å¸¸ç”¨çŸ¥è¯†å›ºåŒ–åˆ°è®°å¿†æ–‡ä»¶",
                }
            )

    # æ˜¯å¦éœ€è¦å‘Šè­¦
    needs_alert = miss_count >= ALERT_THRESHOLD or len(high_freq) > 0

    # â”€â”€ ä¿®å¤å‘½ä¸­ç‡è¿½è¸ª â”€â”€
    hit_rate = _compute_hit_rate(gap_topics)

    # è½ç›˜å½“å‰å»ºè®®ï¼ˆä¾›åç»­å‘½ä¸­ç‡è®¡ç®—ï¼‰
    _log_suggestions(suggestions)

    return {
        "days": days,
        "read_count": read_count,
        "write_count": write_count,
        "miss_count": miss_count,
        "ratio": round(ratio, 1),
        "gap_topics": dict(gap_topics),
        "high_freq_gaps": high_freq,
        "read_only_topics": sorted(read_only),
        "suggestions": suggestions,
        "needs_alert": needs_alert,
        "memory_info": scan_memory_files(),
        "hit_rate": hit_rate,
    }


def format_report(result: dict, compact: bool = False) -> str:
    now = time.strftime("%Y-%m-%d %H:%M")

    if compact:
        lines = [
            f"ğŸ§  è®°å¿†ç›²åŒºåˆ†æ | {now}",
            f"çª—å£: {result['days']}å¤©",
            f"è¯»{result['read_count']} å†™{result['write_count']} ç›²åŒº{result['miss_count']} (è¯»å†™æ¯”{result['ratio']}:1)",
        ]

        if result["high_freq_gaps"]:
            lines.append("")
            lines.append("âš ï¸ é«˜é¢‘ç›²åŒº:")
            for topic, count in sorted(
                result["high_freq_gaps"].items(), key=lambda x: -x[1]
            ):
                lines.append(f"  ğŸ”´ {topic} ({count}æ¬¡)")

        if result["suggestions"]:
            lines.append("")
            lines.append("ğŸ“‹ ä¿®å¤å»ºè®®:")
            for s in result["suggestions"][:5]:
                icon = (
                    "ğŸ”´"
                    if s["priority"] == "é«˜"
                    else "ğŸŸ¡" if s["priority"] == "ä¸­" else "ğŸ”µ"
                )
                lines.append(f"  {icon} {s['action']}")

        if not result["gap_topics"]:
            lines.append("\nâœ… æ— è®°å¿†ç›²åŒº")

        # å‘½ä¸­ç‡
        hr = result.get("hit_rate", {})
        if hr.get("total_suggested", 0) > 0:
            lines.append(
                f"\nğŸ“ˆ ä¿®å¤å‘½ä¸­ç‡: {hr['hit_rate_pct']}% ({hr['fixed']}ä¿®å¤/{hr['total_suggested']}å»ºè®®, {hr['still_open']}æœªä¿®)"
            )

        return "\n".join(lines)

    # å®Œæ•´ Markdown ç‰ˆ
    lines = [
        f"# ğŸ§  AIOS è®°å¿†ç›²åŒºåˆ†ææŠ¥å‘Š",
        f"ç”Ÿæˆæ—¶é—´: {now} | çª—å£: {result['days']}å¤©",
        "",
        "## 1. è®°å¿†æ¦‚å†µ",
        "",
        f"| æŒ‡æ ‡ | å€¼ |",
        f"| :--- | ---: |",
        f"| çŸ¥è¯†æå– (Read) | {result['read_count']} |",
        f"| çŸ¥è¯†å›ºåŒ– (Write) | {result['write_count']} |",
        f"| çŸ¥è¯†ç›²åŒº (Miss) | {result['miss_count']} |",
        f"| è¯»å†™æ¯” | {result['ratio']}:1 |",
    ]

    mi = result["memory_info"]
    lines.extend(
        [
            "",
            "## 2. è®°å¿†æ–‡ä»¶çŠ¶æ€",
            "",
            f"- MEMORY.md: {'âœ…' if mi['memory_md_exists'] else 'âŒ'} ({mi['memory_md_size']} bytes)",
            f"- æ—¥å¿—æ–‡ä»¶: {len(mi['daily_files'])} ä¸ª ({mi['total_daily_size']} bytes)",
            f"- lessons.json: {'âœ…' if mi['lessons_exists'] else 'âŒ'}",
            f"- corrections.json: {'âœ…' if mi['corrections_exists'] else 'âŒ'}",
        ]
    )

    if result["gap_topics"]:
        lines.extend(
            [
                "",
                "## 3. ç›²åŒºè¯¦æƒ…",
                "",
                "| ä¸»é¢˜ | æ¬¡æ•° | ä¼˜å…ˆçº§ |",
                "| :--- | ---: | :--- |",
            ]
        )
        for topic, count in sorted(result["gap_topics"].items(), key=lambda x: -x[1]):
            pri = "ğŸ”´ é«˜" if count >= REPEAT_THRESHOLD else "ğŸŸ¡ ä¸­"
            lines.append(f"| {topic} | {count} | {pri} |")

    if result["read_only_topics"]:
        lines.extend(
            [
                "",
                "## 4. æ½œåœ¨ç›²åŒºï¼ˆè¯»å¤šå†™å°‘ï¼‰",
                "",
            ]
        )
        for t in result["read_only_topics"][:10]:
            lines.append(f"- {t}")

    if result["suggestions"]:
        lines.extend(
            [
                "",
                "## 5. ä¿®å¤å»ºè®®",
                "",
            ]
        )
        for s in result["suggestions"]:
            icon = (
                "ğŸ”´"
                if s["priority"] == "é«˜"
                else "ğŸŸ¡" if s["priority"] == "ä¸­" else "ğŸ”µ"
            )
            lines.append(f"- {icon} [{s['priority']}] {s['action']}")
            lines.append(f"  åŸå› : {s['reason']}")

    if not result["gap_topics"] and not result["read_only_topics"]:
        lines.extend(
            ["", "âœ… è¿‡å» {} å¤©æ— è®°å¿†ç›²åŒºï¼ŒçŸ¥è¯†è¦†ç›–è‰¯å¥½".format(result["days"])]
        )

    # ä¿®å¤å‘½ä¸­ç‡
    hr = result.get("hit_rate", {})
    if hr.get("total_suggested", 0) > 0:
        lines.extend(
            [
                "",
                "## 6. ä¿®å¤å‘½ä¸­ç‡",
                "",
                f"| æŒ‡æ ‡ | å€¼ |",
                f"| :--- | ---: |",
                f"| å†å²å»ºè®®æ•° | {hr['total_suggested']} |",
                f"| å·²ä¿®å¤ | {hr['fixed']} |",
                f"| æœªä¿®å¤ | {hr['still_open']} |",
                f"| å‘½ä¸­ç‡ | {hr['hit_rate_pct']}% |",
            ]
        )

    lines.extend(
        [
            "",
            "---",
            f"*Generated by AIOS Memory Gaps v1.0 | {now}*",
        ]
    )

    return "\n".join(lines)


def main():
    sys.stdout.reconfigure(encoding="utf-8")
    p = argparse.ArgumentParser(description="AIOS è®°å¿†ç›²åŒºåˆ†æ")
    p.add_argument("--days", type=int, default=7, help="åˆ†æçª—å£ï¼ˆå¤©ï¼‰")
    p.add_argument("--format", choices=["markdown", "telegram"], default="markdown")
    p.add_argument("--save", action="store_true", help="ä¿å­˜æŠ¥å‘Š")
    args = p.parse_args()

    result = analyze_gaps(args.days)
    report = format_report(result, compact=(args.format == "telegram"))
    print(report)

    if result["needs_alert"]:
        print("\nâš ï¸ ç›²åŒºè¶…é˜ˆå€¼ï¼Œå»ºè®®å°½å¿«ä¿®å¤ï¼")

    if args.save:
        REPORTS_DIR.mkdir(parents=True, exist_ok=True)
        ts = datetime.now().strftime("%Y%m%d_%H%M")
        out = REPORTS_DIR / f"memory_gaps_{ts}.md"
        out.write_text(report, encoding="utf-8")
        print(f"\nğŸ’¾ å·²ä¿å­˜: {out}")


if __name__ == "__main__":
    main()
