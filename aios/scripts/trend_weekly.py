#!/usr/bin/env python3
"""
AIOS å‘¨è¶‹åŠ¿åˆ†æ v1.0
é€æ—¥æŒ‡æ ‡å¿«ç…§ + é”™è¯¯æ”¶æ•›/å‘æ•£åˆ†æ + æ–¹å‘ç®­å¤´

ç”¨æ³•:
  python -m aios.scripts.trend_weekly                    # è¿‡å»7å¤©å‘¨æŠ¥
  python -m aios.scripts.trend_weekly --days 14          # è‡ªå®šä¹‰å¤©æ•°
  python -m aios.scripts.trend_weekly --format telegram  # ç²¾ç®€ç‰ˆ
  python -m aios.scripts.trend_weekly --save             # ä¿å­˜åˆ° reports/
"""

import json, math, sys, time, argparse
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from pathlib import Path

sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from core.engine import load_events, VALID_LAYERS
from core.config import get_path

REPORTS_DIR = Path(__file__).resolve().parent.parent / "reports"


# â”€â”€ å·¥å…·å‡½æ•° â”€â”€


def _day_key(epoch: float) -> str:
    return time.strftime("%m-%d", time.localtime(epoch))


def _day_full(epoch: float) -> str:
    return time.strftime("%Y-%m-%d", time.localtime(epoch))


def _payload(e: dict) -> dict:
    return e.get("payload", e.get("data", {}))


def _layer(e: dict) -> str:
    l = e.get("layer", "")
    if l in VALID_LAYERS:
        return l
    t = e.get("type", "")
    mapping = {
        "tool": "TOOL",
        "task": "TOOL",
        "match": "MEM",
        "correction": "MEM",
        "error": "SEC",
        "http_error": "SEC",
        "health": "KERNEL",
        "deploy": "KERNEL",
    }
    return mapping.get(t, "TOOL")


def _is_ok(e: dict) -> bool:
    if e.get("status") == "err":
        return False
    return _payload(e).get("ok", True)


def _event_name(e: dict) -> str:
    return e.get("event", _payload(e).get("_v1_type", e.get("type", "?")))


def _latency(e: dict) -> int:
    ms = e.get("latency_ms", 0)
    if ms:
        return ms
    p = _payload(e)
    return p.get("ms", p.get("elapsed_ms", p.get("duration_ms", 0)))


def _p95(values: list) -> int:
    if not values:
        return 0
    s = sorted(values)
    return s[math.ceil(0.95 * len(s)) - 1] if len(s) >= 2 else s[0]


def _trend_arrow(values: list) -> str:
    """æ ¹æ®åºåˆ—è¶‹åŠ¿è¿”å›æ–¹å‘ç®­å¤´"""
    if len(values) < 2:
        return "â†’"
    first_half = sum(values[: len(values) // 2]) / max(len(values) // 2, 1)
    second_half = sum(values[len(values) // 2 :]) / max(
        len(values) - len(values) // 2, 1
    )
    if first_half == 0:
        return "â†’" if second_half == 0 else "â†‘"
    pct = (second_half - first_half) / first_half * 100
    if pct > 15:
        return "â†‘"
    elif pct < -15:
        return "â†“"
    return "â†’"


def _sparkline(values: list) -> str:
    """ç®€æ˜“ sparklineï¼ˆç”¨ Unicode block å­—ç¬¦ï¼‰"""
    if not values:
        return ""
    blocks = " â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆ"
    mn, mx = min(values), max(values)
    rng = mx - mn if mx != mn else 1
    return "".join(blocks[min(int((v - mn) / rng * 8), 8)] for v in values)


# â”€â”€ æ ¸å¿ƒåˆ†æ â”€â”€


def compute_daily_metrics(events: list, days: int) -> list:
    """æŒ‰å¤©åˆ†æ¡¶ï¼Œè®¡ç®—æ¯å¤©çš„æŒ‡æ ‡"""
    now = time.time()
    buckets = defaultdict(list)
    for e in events:
        epoch = e.get("epoch", 0)
        if epoch <= 0:
            continue
        buckets[_day_full(epoch)].append(e)

    # ç”Ÿæˆè¿ç»­æ—¥æœŸåºåˆ—
    result = []
    for i in range(days - 1, -1, -1):
        day = (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d")
        day_events = buckets.get(day, [])
        total = len(day_events)

        # TSR
        tools = [e for e in day_events if _layer(e) == "TOOL"]
        tool_ok = sum(1 for t in tools if _is_ok(t))
        tsr = tool_ok / len(tools) * 100 if tools else 100.0

        # å±‚åˆ†å¸ƒ
        layer_counts = Counter(_layer(e) for e in day_events)

        # å»¶è¿Ÿ
        latencies = [_latency(e) for e in day_events if _latency(e) > 0]
        avg_lat = round(sum(latencies) / len(latencies)) if latencies else 0
        p95_lat = _p95(latencies)

        # é”™è¯¯äº‹ä»¶
        errors = [e for e in day_events if not _is_ok(e)]
        error_types = Counter(_event_name(e) for e in errors)

        # MEM ç›²åŒº
        mem_events = [e for e in day_events if _layer(e) == "MEM"]
        misses = sum(1 for e in mem_events if "miss" in _event_name(e))

        result.append(
            {
                "date": day,
                "date_short": day[5:],  # MM-DD
                "total": total,
                "tsr": round(tsr, 1),
                "layer_counts": dict(layer_counts),
                "avg_latency": avg_lat,
                "p95_latency": p95_lat,
                "errors": dict(error_types),
                "error_count": len(errors),
                "mem_misses": misses,
            }
        )

    return result


def analyze_error_convergence(daily: list) -> list:
    """åˆ†æé”™è¯¯ç±»å‹çš„æ”¶æ•›/å‘æ•£è¶‹åŠ¿"""
    # æ”¶é›†æ‰€æœ‰å‡ºç°è¿‡çš„é”™è¯¯ç±»å‹
    all_error_types = set()
    for d in daily:
        all_error_types.update(d["errors"].keys())

    results = []
    for err_type in sorted(all_error_types):
        counts = [d["errors"].get(err_type, 0) for d in daily]
        total = sum(counts)
        trend = _trend_arrow(counts)

        # åˆ¤æ–­çŠ¶æ€
        if total == 0:
            status = "å·²æ¶ˆé™¤"
        elif trend == "â†“":
            status = "æ”¶æ•›ä¸­"
        elif trend == "â†‘":
            status = "å‘æ•£ä¸­"
        else:
            status = "ç¨³å®š"

        results.append(
            {
                "type": err_type,
                "total": total,
                "trend": trend,
                "status": status,
                "daily": counts,
                "spark": _sparkline(counts),
            }
        )

    # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åºï¼šå‘æ•£ > ç¨³å®š > æ”¶æ•› > å·²æ¶ˆé™¤
    priority = {"å‘æ•£ä¸­": 0, "ç¨³å®š": 1, "æ”¶æ•›ä¸­": 2, "å·²æ¶ˆé™¤": 3}
    results.sort(key=lambda x: (priority.get(x["status"], 9), -x["total"]))
    return results


def generate_weekly_report(days: int = 7, compact: bool = False) -> str:
    events = load_events(days)
    daily = compute_daily_metrics(events, days)
    error_conv = analyze_error_convergence(daily)

    now = time.strftime("%Y-%m-%d %H:%M")
    total_events = sum(d["total"] for d in daily)

    # è¶‹åŠ¿æ•°æ®
    tsr_values = [d["tsr"] for d in daily if d["total"] > 0]
    vol_values = [d["total"] for d in daily]
    lat_values = [d["avg_latency"] for d in daily if d["avg_latency"] > 0]
    miss_values = [d["mem_misses"] for d in daily]

    tsr_trend = _trend_arrow(tsr_values)
    vol_trend = _trend_arrow(vol_values)
    lat_trend = _trend_arrow(lat_values)
    miss_trend = _trend_arrow(miss_values)

    avg_tsr = round(sum(tsr_values) / len(tsr_values), 1) if tsr_values else 100.0
    avg_vol = round(sum(vol_values) / len(vol_values), 1)

    if compact:
        # Telegram ç²¾ç®€ç‰ˆ
        lines = [
            f"ğŸ“Š AIOS å‘¨è¶‹åŠ¿ | {now}",
            f"çª—å£: {days}å¤© | æ€»äº‹ä»¶: {total_events}",
            "",
            f"TSR: {avg_tsr}% {tsr_trend} {_sparkline(tsr_values)}",
            f"äº‹ä»¶é‡: æ—¥å‡{avg_vol:.0f} {vol_trend} {_sparkline(vol_values)}",
        ]
        if lat_values:
            avg_lat = round(sum(lat_values) / len(lat_values))
            lines.append(f"å»¶è¿Ÿ: å‡{avg_lat}ms {lat_trend}")
        lines.append(f"è®°å¿†ç›²åŒº: {sum(miss_values)}æ¬¡ {miss_trend}")

        # é”™è¯¯æ”¶æ•›
        diverging = [e for e in error_conv if e["status"] == "å‘æ•£ä¸­"]
        converging = [e for e in error_conv if e["status"] == "æ”¶æ•›ä¸­"]
        if diverging:
            lines.append("")
            lines.append("âš ï¸ å‘æ•£ä¸­çš„é”™è¯¯:")
            for e in diverging[:5]:
                lines.append(
                    f"  {e['trend']} {e['type']} ({e['total']}æ¬¡) {e['spark']}"
                )
        if converging:
            lines.append("")
            lines.append("âœ… æ”¶æ•›ä¸­çš„é”™è¯¯:")
            for e in converging[:5]:
                lines.append(
                    f"  {e['trend']} {e['type']} ({e['total']}æ¬¡) {e['spark']}"
                )

        if not error_conv:
            lines.append("\nâœ… æ— é”™è¯¯äº‹ä»¶")

        return "\n".join(lines)

    # å®Œæ•´ Markdown ç‰ˆ
    lines = [
        f"# ğŸ“Š AIOS å‘¨è¶‹åŠ¿æŠ¥å‘Š",
        f"ç”Ÿæˆæ—¶é—´: {now} | çª—å£: {days}å¤© | æ€»äº‹ä»¶: {total_events}",
        "",
        "## 1. å…³é”®æŒ‡æ ‡è¶‹åŠ¿",
        "",
        f"| æŒ‡æ ‡ | å‡å€¼ | è¶‹åŠ¿ | ç«èŠ±å›¾ |",
        f"| :--- | ---: | :---: | :--- |",
        f"| TSR | {avg_tsr}% | {tsr_trend} | {_sparkline(tsr_values)} |",
        f"| æ—¥äº‹ä»¶é‡ | {avg_vol:.0f} | {vol_trend} | {_sparkline(vol_values)} |",
    ]
    if lat_values:
        avg_lat = round(sum(lat_values) / len(lat_values))
        lines.append(
            f"| å¹³å‡å»¶è¿Ÿ | {avg_lat}ms | {lat_trend} | {_sparkline(lat_values)} |"
        )
    lines.append(
        f"| è®°å¿†ç›²åŒº | {sum(miss_values)}æ¬¡ | {miss_trend} | {_sparkline(miss_values)} |"
    )

    # é€æ—¥æ˜ç»†
    lines.extend(
        [
            "",
            "## 2. é€æ—¥æ˜ç»†",
            "",
            "| æ—¥æœŸ | äº‹ä»¶ | TSR | é”™è¯¯ | ç›²åŒº | å»¶è¿Ÿ(avg) |",
            "| :--- | ---: | ---: | ---: | ---: | ---: |",
        ]
    )
    for d in daily:
        lines.append(
            f"| {d['date_short']} | {d['total']} | {d['tsr']}% | "
            f"{d['error_count']} | {d['mem_misses']} | {d['avg_latency']}ms |"
        )

    # é”™è¯¯æ”¶æ•›åˆ†æ
    lines.extend(
        [
            "",
            "## 3. é”™è¯¯æ”¶æ•›åˆ†æ",
            "",
        ]
    )
    if error_conv:
        lines.append("| é”™è¯¯ç±»å‹ | æ€»æ¬¡æ•° | è¶‹åŠ¿ | çŠ¶æ€ | åˆ†å¸ƒ |")
        lines.append("| :--- | ---: | :---: | :--- | :--- |")
        for e in error_conv:
            lines.append(
                f"| {e['type']} | {e['total']} | {e['trend']} | {e['status']} | {e['spark']} |"
            )
    else:
        lines.append("âœ… è¿‡å» {days} å¤©æ— é”™è¯¯äº‹ä»¶")

    # å±‚åˆ†å¸ƒè¶‹åŠ¿
    lines.extend(
        [
            "",
            "## 4. å±‚åˆ†å¸ƒè¶‹åŠ¿",
            "",
        ]
    )
    for layer_name in ["KERNEL", "COMMS", "TOOL", "MEM", "SEC"]:
        vals = [d["layer_counts"].get(layer_name, 0) for d in daily]
        lines.append(
            f"- {layer_name}: {_sparkline(vals)} {_trend_arrow(vals)} (æ€»{sum(vals)})"
        )

    lines.extend(
        [
            "",
            "---",
            f"*Generated by AIOS Trend Weekly v1.0 | {now}*",
        ]
    )

    return "\n".join(lines)


def main():
    sys.stdout.reconfigure(encoding="utf-8")
    p = argparse.ArgumentParser(description="AIOS å‘¨è¶‹åŠ¿åˆ†æ")
    p.add_argument("--days", type=int, default=7, help="åˆ†æçª—å£ï¼ˆå¤©ï¼‰")
    p.add_argument("--format", choices=["markdown", "telegram"], default="markdown")
    p.add_argument("--save", action="store_true", help="ä¿å­˜åˆ° reports/")
    args = p.parse_args()

    report = generate_weekly_report(args.days, compact=(args.format == "telegram"))
    print(report)

    if args.save:
        REPORTS_DIR.mkdir(parents=True, exist_ok=True)
        ts = datetime.now().strftime("%Y%m%d_%H%M")
        out = REPORTS_DIR / f"trend_weekly_{ts}.md"
        out.write_text(report, encoding="utf-8")
        print(f"\nğŸ’¾ å·²ä¿å­˜: {out}")


if __name__ == "__main__":
    main()
