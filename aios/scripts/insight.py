# aios/scripts/insight.py - ç©·äººç‰ˆ ClickHouseï¼šæ¯æ—¥å¥åº·ç®€æŠ¥
"""
è¯»å– events.jsonlï¼Œç”Ÿæˆ Markdown ç®€æŠ¥ã€‚
ä¸ä¾èµ– Pandasï¼Œä¸ä¾èµ–å¤–éƒ¨æœåŠ¡ï¼Œçº¯ Python åŸç”Ÿã€‚

ç”¨æ³•ï¼š
  python -m aios.scripts.insight          # è¿‡å» 24h
  python -m aios.scripts.insight --days 7 # è¿‡å» 7 å¤©
  python -m aios.scripts.insight --out telegram  # è¾“å‡ºåˆ° telegramï¼ˆç²¾ç®€ç‰ˆï¼‰
"""

import json, math, sys, time
from collections import Counter, defaultdict
from pathlib import Path

sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from core.engine import load_events, count_by_layer, VALID_LAYERS
from core.config import get_path

LEARNING_DIR = Path(__file__).resolve().parent.parent / "learning"


def _p95(values: list) -> int:
    if not values:
        return 0
    s = sorted(values)
    return s[math.ceil(0.95 * len(s)) - 1] if len(s) >= 2 else s[0]


def _p50(values: list) -> int:
    if not values:
        return 0
    s = sorted(values)
    return s[len(s) // 2]


def _event_name(e: dict) -> str:
    return e.get("event", (e.get("payload") or {}).get("_v1_type", e.get("type", "?")))


def _payload(e: dict) -> dict:
    return e.get("payload", e.get("data", {}))


def _latency(e: dict) -> int:
    ms = e.get("latency_ms", 0)
    if ms:
        return ms
    p = _payload(e)
    return p.get("ms", p.get("elapsed_ms", p.get("duration_ms", 0)))


def _is_ok(e: dict) -> bool:
    if e.get("status") == "err":
        return False
    return _payload(e).get("ok", True)


def _layer(e: dict) -> str:
    l = e.get("layer", "")
    if l in VALID_LAYERS:
        return l
    # v0.1 å…¼å®¹
    t = e.get("type", "")
    mapping = {
        "tool": "TOOL",
        "task": "TOOL",
        "match": "MEM",
        "correction": "MEM",
        "error": "SEC",
        "http_error": "SEC",
        "health": "KERNEL",
        "deploy": "KERNEL",
    }
    return mapping.get(t, "TOOL")


def generate_insight(days: int = 1, compact: bool = False) -> str:
    events = load_events(days)
    if not events:
        return "è¿‡å» {}h æ— äº‹ä»¶æ•°æ®ã€‚".format(days * 24)

    now = time.strftime("%Y-%m-%d %H:%M", time.localtime())
    total = len(events)

    # â”€â”€ 1. æŒ‰å±‚åˆ†ç±» â”€â”€
    by_layer = defaultdict(list)
    for e in events:
        by_layer[_layer(e)].append(e)

    # â”€â”€ 2. KERNEL åˆ†æï¼šå¿ƒè·³ â”€â”€
    kernel = by_layer["KERNEL"]
    loops = [e for e in kernel if "loop" in _event_name(e)]
    token_events = [e for e in kernel if "token" in _event_name(e)]
    total_input_tokens = sum(_payload(e).get("input_tokens", 0) for e in token_events)
    total_output_tokens = sum(_payload(e).get("output_tokens", 0) for e in token_events)
    prune_events = [e for e in kernel if "prune" in _event_name(e)]

    # â”€â”€ 3. TOOL åˆ†æï¼šæ‰‹è„šæ•ˆç‡ â”€â”€
    tools = by_layer["TOOL"]
    tool_ok = sum(1 for t in tools if _is_ok(t))
    tool_err = len(tools) - tool_ok
    tsr = tool_ok / len(tools) * 100 if tools else 100

    by_tool_name = defaultdict(list)
    for e in tools:
        p = _payload(e)
        name = p.get("name", _event_name(e))
        ms = _latency(e)
        if ms > 0:
            by_tool_name[name].append(ms)

    tool_stats = []
    for name, times in sorted(by_tool_name.items(), key=lambda x: -_p95(x[1])):
        tool_stats.append(
            {
                "name": name,
                "calls": len(times),
                "p50": _p50(times),
                "p95": _p95(times),
                "avg": round(sum(times) / len(times)),
            }
        )

    all_latencies = [ms for times in by_tool_name.values() for ms in times]
    global_p95 = _p95(all_latencies)
    global_avg = round(sum(all_latencies) / len(all_latencies)) if all_latencies else 0

    # â”€â”€ 4. SEC åˆ†æï¼šå…ç–«ååº” â”€â”€
    sec = by_layer["SEC"]
    sec_by_event = Counter(_event_name(e) for e in sec)
    critical = [
        e for e in sec if _event_name(e) in ("system_crash", "circuit_breaker_tripped")
    ]

    # åŒºåˆ†æµ‹è¯•äº‹ä»¶ï¼špayload é‡Œæœ‰ sig="sig_abc" æˆ– test=True çš„è§†ä¸ºæµ‹è¯•
    test_critical = [
        e
        for e in critical
        if _payload(e).get("sig") == "sig_abc" or _payload(e).get("test")
    ]
    real_critical = [e for e in critical if e not in test_critical]

    # â”€â”€ 5. MEM åˆ†æï¼šè®°å¿†æ·±åº¦ â”€â”€
    mem = by_layer["MEM"]
    mem_reads = [
        e
        for e in mem
        if any(
            k in _event_name(e) for k in ("recall", "match", "confirm", "load", "miss")
        )
    ]
    mem_writes = [
        e
        for e in mem
        if any(k in _event_name(e) for k in ("store", "correction", "lesson"))
    ]
    mem_misses = [e for e in mem if "miss" in _event_name(e)]

    # â”€â”€ 6. COMMS åˆ†æï¼šå¯¹è¯ â”€â”€
    comms = by_layer["COMMS"]
    user_inputs = [e for e in comms if "user_input" in _event_name(e)]
    agent_responses = [e for e in comms if "agent_response" in _event_name(e)]
    response_latencies = [_latency(e) for e in agent_responses if _latency(e) > 0]
    avg_response_ms = (
        round(sum(response_latencies) / len(response_latencies))
        if response_latencies
        else 0
    )

    # â”€â”€ 7. è®¤çŸ¥æ­»å¾ªç¯æ£€æµ‹ï¼ˆä¼˜åŒ–ï¼šæ’é™¤éƒ¨ç½²çª—å£ï¼‰â”€â”€
    # è¿ç»­ 5+ ä¸ª KERNEL äº‹ä»¶æ²¡æœ‰ TOOL äº§å‡º = å¯èƒ½å¡ä½äº†
    # ä½†æ’é™¤ deploy/restart/rollout ç­‰æ­£å¸¸æ‰¹é‡äº‹ä»¶
    deadlock_warnings = 0
    excluded_deploy_restart = 0
    consecutive_kernel = 0
    consecutive_kernel_events = []

    for e in events:
        layer = _layer(e)
        event_name = _event_name(e)

        if layer == "KERNEL":
            consecutive_kernel += 1
            consecutive_kernel_events.append(e)
        elif layer == "TOOL":
            consecutive_kernel = 0
            consecutive_kernel_events = []

        if consecutive_kernel >= 5:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ­£å¸¸çª—å£ï¼š
            # 1. éƒ¨ç½²/é‡å¯/å‘å¸ƒ
            # 2. èµ„æºå¿«ç…§/åæ€
            # 3. Scheduler å†³ç­–
            # 4. ä»»åŠ¡åˆ›å»º
            # 5. æµ‹è¯•äº‹ä»¶ï¼ˆnoop/sleep/failï¼‰
            is_normal_window = all(
                any(
                    k in _event_name(ev).lower()
                    for k in (
                        "deploy", "restart", "rollout",
                        "resource_snapshot", "reflection",
                        "scheduler.decision", "task.", "noop", "sleep", "fail"
                    )
                )
                for ev in consecutive_kernel_events
            )

            if is_normal_window:
                excluded_deploy_restart += 1
            else:
                deadlock_warnings += 1

            consecutive_kernel = 0
            consecutive_kernel_events = []

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  ç”ŸæˆæŠ¥å‘Š
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    if compact:
        # Telegram ç²¾ç®€ç‰ˆ
        lines = [
            f"ğŸ“Š AIOS ç®€æŠ¥ | {now}",
            f"äº‹ä»¶: {total} | TSR: {tsr:.1f}%",
            f"å±‚åˆ†å¸ƒ: K{len(kernel)} C{len(comms)} T{len(tools)} M{len(mem)} S{len(sec)}",
        ]
        if tool_stats:
            slowest = tool_stats[0]
            lines.append(f"æœ€æ…¢: {slowest['name']} p95={slowest['p95']}ms")
        if real_critical:
            lines.append(f"âš ï¸ è‡´å‘½äº‹ä»¶: {len(real_critical)}")
        if test_critical:
            lines.append(
                f"âš ï¸ è‡´å‘½äº‹ä»¶(å«æµ‹è¯•): {len(critical)} (æµ‹è¯•{len(test_critical)})"
            )
        if deadlock_warnings:
            lines.append(f"âš ï¸ ç–‘ä¼¼æ­»å¾ªç¯: {deadlock_warnings}")
        if excluded_deploy_restart:
            lines.append(f"â„¹ï¸ å·²æ’é™¤éƒ¨ç½²çª—å£: {excluded_deploy_restart}")
        lines.append(
            f"è®°å¿†: è¯»{len(mem_reads)} å†™{len(mem_writes)} ç›²åŒº{len(mem_misses)}"
        )
        if total_input_tokens + total_output_tokens > 0:
            lines.append(f"Token: å…¥{total_input_tokens} å‡º{total_output_tokens}")
        return "\n".join(lines)

    # å®Œæ•´ Markdown ç‰ˆ
    lines = [
        f"# ğŸ¤– AIOS æ¯æ—¥å¥åº·ç®€æŠ¥",
        f"æ—¥æœŸ: {now} | çª—å£: {days * 24}h | æ€»äº‹ä»¶: {total}",
        "",
        "## 1. ç¥ç»ç³»ç»Ÿåˆ†å¸ƒ",
        "",
        "| å±‚çº§ | äº‹ä»¶æ•° | å æ¯” |",
        "| :--- | ---: | ---: |",
    ]
    for layer_name in ["KERNEL", "COMMS", "TOOL", "MEM", "SEC"]:
        count = len(by_layer[layer_name])
        pct = count / total * 100 if total else 0
        lines.append(f"| {layer_name} | {count} | {pct:.1f}% |")

    lines.extend(
        [
            "",
            "## 2. ç”Ÿå‘½ä½“å¾ (KERNEL)",
            f"- å¾ªç¯æ¬¡æ•°: {len(loops)}",
            f"- Token æ¶ˆè€—: è¾“å…¥ {total_input_tokens:,} + è¾“å‡º {total_output_tokens:,} = {total_input_tokens + total_output_tokens:,}",
            f"- ä¸Šä¸‹æ–‡è£å‰ª: {len(prune_events)} æ¬¡",
        ]
    )
    if deadlock_warnings:
        lines.append(f"- âš ï¸ ç–‘ä¼¼è®¤çŸ¥æ­»å¾ªç¯: {deadlock_warnings} æ¬¡")

    lines.extend(
        [
            "",
            "## 3. è‚¢ä½“æ•ˆèƒ½ (TOOL)",
            f"- ä»»åŠ¡æˆåŠŸç‡ (TSR): {tsr:.1f}% ({tool_ok}âœ“ / {tool_err}âœ—)",
            f"- å…¨å±€å»¶è¿Ÿ: avg={global_avg}ms p95={global_p95}ms",
            "",
        ]
    )
    if tool_stats:
        lines.append("| å·¥å…· | è°ƒç”¨ | p50 | p95 | avg |")
        lines.append("| :--- | ---: | ---: | ---: | ---: |")
        for ts in tool_stats[:10]:
            flag = " ğŸŒ" if ts["p95"] > 5000 else ""
            lines.append(
                f"| {ts['name']}{flag} | {ts['calls']} | {ts['p50']}ms | {ts['p95']}ms | {ts['avg']}ms |"
            )

    lines.extend(
        [
            "",
            "## 4. å…ç–«ååº” (SEC)",
            f"- å®‰å…¨äº‹ä»¶: {len(sec)} æ¡",
        ]
    )
    if sec_by_event:
        for evt, cnt in sec_by_event.most_common(5):
            lines.append(f"  - {evt}: {cnt}")
    if critical:
        lines.append(f"- ğŸš¨ è‡´å‘½äº‹ä»¶: {len(critical)} æ¡")
    if not sec:
        lines.append("- âœ… ç³»ç»Ÿå¹³ç¨³ï¼Œæ— å¼‚å¸¸")

    lines.extend(
        [
            "",
            "## 5. è®¤çŸ¥è®°å¿† (MEM)",
            f"- çŸ¥è¯†æå– (Read): {len(mem_reads)} æ¬¡",
            f"- çŸ¥è¯†å›ºåŒ– (Write): {len(mem_writes)} æ¬¡",
            f"- çŸ¥è¯†ç›²åŒº (Miss): {len(mem_misses)} æ¬¡",
        ]
    )
    if mem_reads or mem_writes:
        ratio = len(mem_reads) / max(len(mem_writes), 1)
        if ratio > 5:
            lines.append("- ğŸ“– æ¨¡å¼: ä»¥æ£€ç´¢ä¸ºä¸»ï¼ˆç»éªŒä¸°å¯Œï¼ŒæŸ¥é˜…å¤šäºå­¦ä¹ ï¼‰")
        elif ratio > 1:
            lines.append("- âš–ï¸ æ¨¡å¼: è¯»å†™å‡è¡¡ï¼ˆè¾¹å­¦è¾¹ç”¨ï¼‰")
        else:
            lines.append("- âœï¸ æ¨¡å¼: ä»¥å­¦ä¹ ä¸ºä¸»ï¼ˆæ–°çŸ¥è¯†å¯†é›†æœŸï¼‰")

    lines.extend(
        [
            "",
            "## 6. å¯¹è¯è´¨é‡ (COMMS)",
            f"- ç”¨æˆ·è¾“å…¥: {len(user_inputs)} æ¡",
            f"- Agent å›å¤: {len(agent_responses)} æ¡",
            f"- å¹³å‡å“åº”å»¶è¿Ÿ: {avg_response_ms}ms",
        ]
    )

    lines.extend(
        [
            "",
            "---",
            "*Generated by AIOS Insight v0.2*",
        ]
    )

    return "\n".join(lines)


def main():
    import argparse

    sys.stdout.reconfigure(encoding="utf-8")
    p = argparse.ArgumentParser(description="AIOS æ¯æ—¥å¥åº·ç®€æŠ¥")
    p.add_argument("--days", type=int, default=1, help="åˆ†æçª—å£ï¼ˆå¤©ï¼‰")
    p.add_argument(
        "--out", choices=["markdown", "telegram"], default="markdown", help="è¾“å‡ºæ ¼å¼"
    )
    p.add_argument("--save", action="store_true", help="ä¿å­˜åˆ°æ–‡ä»¶")
    args = p.parse_args()

    compact = args.out == "telegram"
    report = generate_insight(args.days, compact)
    print(report)

    if args.save and not compact:
        date_str = time.strftime("%Y-%m-%d")
        out_path = LEARNING_DIR / f"insight_{date_str}.md"
        out_path.write_text(report, encoding="utf-8")
        print(f"\nå·²ä¿å­˜åˆ°: {out_path}")


if __name__ == "__main__":
    main()
